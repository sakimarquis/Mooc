{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw1XwPRD9OLu"
   },
   "source": [
    "# IERG 5350 Assignment 4: Advanced Algorithms for Continuous Control in RL\n",
    "\n",
    "### Welcome to assignment 4 of our RL course!\n",
    "*2020-2021 Term 1, IERG 5350: Reinforcement Learning. Department of Information Engineering, The Chinese University of Hong Kong. Course Instructor: Professor ZHOU Bolei. Assignment author: PENG Zhenghao, SUN Hao, ZHAN Xiaohang.*\n",
    "\n",
    "\n",
    "| Student Name | Student ID |\n",
    "| :----: | :----: |\n",
    "| TYPE_YOUR_NAME_HERE | TYPE_YOUR_STUDENT_ID_HERE |\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "In this assignment, we will implement a system of RL that allows us to train and evaluate RL agents formally and efficiently.\n",
    "\n",
    "In this notebook, you will go through the following components of the whole system:\n",
    "- Preparation: Colab, and Environment\n",
    "- Section 1: Training with algorithm PPO\n",
    "- Section 2: Training with algorithm DDPG\n",
    "- Section 3: Training with algorithm TD3\n",
    "- Section 4: Transfer your PPO/ DDPG/ TD3 to another task: Four-Solution-Maze\n",
    "\n",
    "The author of this assignment is SUN, Hao (sh018 AT ie.cuhk.edu.hk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyLEr-yOpxvD"
   },
   "source": [
    "# Colab\n",
    "\n",
    "### Introduction to Google Colab: \n",
    "From now on, our assignment as well as the final project will be based on the Google Colab, where you can apply for free GPU resources to accelerate the learning of your RL models. \n",
    "\n",
    "Here are some resources as intro to the Colab.\n",
    "\n",
    "- YouTube Video: https://www.youtube.com/watch?v=inN8seMm7UI\n",
    "- Colab Intro: https://colab.research.google.com/notebooks/intro.ipynb\n",
    "(you may need to login with your google account)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Gym Continuous Control Tasks\n",
    "\n",
    "### Introduction to the Gym Continuous Control Envirionments\n",
    "\n",
    "In the last assignment, you have already used the gym[atari] benchmarks, where the action space is discrete so that normal approach is value-based methods e.g., DQN.\n",
    "\n",
    "In this assignment, we will try to implement three prevailing RL algorithms for continuous control tasks, namely the PPO(https://arxiv.org/abs/1707.06347), DDPG(https://arxiv.org/abs/1509.02971) and TD3(https://arxiv.org/abs/1802.09477).\n",
    "\n",
    "We will now begin with a gym environment for continuous control,\n",
    "\n",
    "The Pendulum-v0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "2CjdPG_oqT1l",
    "outputId": "541724ca-af57-4bed-817f-728b21e8fb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the state space is like [ 0.62879265  0.77757302 -0.13784444]\n",
      "the max and min action is:  [2.] [-2.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'so that you may need to use action value re-size if you want to use the tanh activation functions'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "ENV_NAME = \"Pendulum-v0\"\n",
    "env = gym.make(ENV_NAME)\n",
    "state = env.reset()\n",
    "print('the state space is like', state)\n",
    "print('the max and min action is: ',env.action_space.high,env.action_space.low)\n",
    "\n",
    "'''so that you may need to use action value re-size if you want to use the tanh activation functions'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7IHoYJWur1S"
   },
   "source": [
    "# PPO \n",
    "\n",
    "The Proximal Policy Optimization Algorithms is the most prevailing on-policy learning method. Although its sample efficiency is not as high as the off-policy methods, the PPO is relatively easy to implement and the learning is much more stable than off-policy methods. Whenever you have a task you want to try whether RL works, you may try to run a PPO agent at first. It is worth mentioning even the most challenging game, the StarCraftII agent AlphaStar is trained based on PPO (with lots of improvements, ofcourse).\n",
    "\n",
    "\n",
    "## TODOs for You\n",
    "The ppo has the benfitsof trust region policy optimization (TRPO) but is much simpler to implement, and with some implementation engeneering, the sample complexity of TRPO is further improved.\n",
    "\n",
    "The key idea of PPO optimization is *Not Optimize the Policy Too Much in a Certain Step*, which follows the key insight of the method of TRPO.\n",
    "\n",
    "In TRPO, the optimization objective of policy is to learn a policy such that \n",
    "\n",
    "$$\\max_\\theta \\hat{\\mathbb{E}}_t [\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\hat{A}_t]$$\n",
    "\n",
    "subject to \n",
    "\n",
    "$$\\hat{\\mathbb{E}}_t[KL[\\pi_{\\theta_{old}}(\\cdot|s_t),\\pi_\\theta(\\cdot|s_t)]] \\le \\delta$$\n",
    "\n",
    "where $\\hat{A}$ denotes the advantage function, rather than optimize the objective function of \n",
    "\n",
    "$$L^{PG}(\\theta) = \\hat{\\mathbb{E}}_t[\\log \\pi_\\theta(a_t|s_t)\\hat{A}_t]$$\n",
    "\n",
    "in the normal policy gradint methods.\n",
    "\n",
    "The PPO proposed two alternative approaches to solve the constrained optimization above, namely the Clipped Surrogated Objective and the Adaptive KL penalty Coefficient. The former one is more generally used in practice as it's more convenient to implement, more efficient and owns stable performance.\n",
    "\n",
    "The Clipped Surrogated Objective approach replace the surrogate objective\n",
    "\n",
    "$$L^{CPI}(\\theta) = \\hat{\\mathbb{E}}_t[\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\hat{A}_t] = \\hat{\\mathbb{E}}_t[r_t(\\theta)\\hat{A}_t]$$\n",
    "\n",
    "of TRPO (CPI: Conservative Policy Iteration) by \n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[\\min(r_t(\\theta)\\hat{A}_t,clip(r_t(\\theta),1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$\n",
    "\n",
    "You can check that $L^{CLIP}(\\theta) = L^{CPI}(\\theta)$ around the old policy parameter $\\theta_{old}$, i.e., when r = 1.\n",
    "\n",
    "## TODOs here:\n",
    "\n",
    "In this section, your task is to finish the code of a PPO algorithm and evaluate its performance in the Pendulum-v0 environment.\n",
    "\n",
    "Specifically, you need to\n",
    "- Q1. finish building up the ActorCritic ''\\__init__'' function, i.e., build up the neural network.\n",
    "- Q2. finish the foward function, in this part, there are two functions need to finish: the \\_forward_actor function and the \\_forward_critic function\n",
    "- Q3. finish the select_action function, which is called during interacting with the environment, so that you may need to return an action as well as the (log-)probability of getting that action for future optimization\n",
    "- Q4. finish the optimization steps for your PPO agent, that means you need to build up the surrogate loss through your saved tuples in previous episodes and optimize it with current network parameters.\n",
    "- Q5. finally, you may need to optimize some of the hyper-parameters to have a better task performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCkX9dWFvXxa"
   },
   "outputs": [],
   "source": [
    "# You need not to rivese this unless you want to try other hyper-parameter settings\n",
    "# in which case you may revise the default values of class args()\n",
    "from IPython import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as joindir\n",
    "from os import makedirs as mkdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'value', 'action', 'logproba', 'mask', 'next_state', 'reward'))\n",
    "env = gym.make(ENV_NAME)\n",
    "env.reset()\n",
    "\n",
    "EPS = 1e-10 # you may need this tiny value somewhere, and think about why?\n",
    "RESULT_DIR = 'Result_PPO'\n",
    "mkdir(RESULT_DIR, exist_ok=True)\n",
    "mkdir(ENV_NAME.split('-')[0]+'/CheckPoints', exist_ok=True)\n",
    "mkdir(ENV_NAME.split('-')[0]+'/Rwds', exist_ok=True)\n",
    "rwds = []\n",
    "rwds_history = []\n",
    "\n",
    "class args(object):\n",
    "    hid_num = 256\n",
    "    drop_prob = 0.1\n",
    "    env_name = ENV_NAME\n",
    "    seed = 1234\n",
    "    num_episode = 1000\n",
    "    batch_size = 5120\n",
    "    max_step_per_round = 2000\n",
    "    gamma = 0.995\n",
    "    lamda = 0.97\n",
    "    log_num_episode = 1\n",
    "    num_epoch = 10\n",
    "    minibatch_size = 256\n",
    "    clip = 0.2\n",
    "    loss_coeff_value = 0.5\n",
    "    loss_coeff_entropy = 0.01\n",
    "    lr = 3e-4\n",
    "    num_parallel_run = 1\n",
    "    # tricks\n",
    "    schedule_adam = 'linear'\n",
    "    schedule_clip = 'linear'\n",
    "    layer_norm = True\n",
    "    state_norm = False\n",
    "    advantage_norm = True\n",
    "    lossvalue_norm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0zwluYyx-y_"
   },
   "outputs": [],
   "source": [
    "# You need not to rivese this, these classes are used for normalization\n",
    "class RunningStat(object):\n",
    "    def __init__(self, shape):\n",
    "        self._n = 0\n",
    "        self._M = np.zeros(shape)\n",
    "        self._S = np.zeros(shape)\n",
    "\n",
    "    def push(self, x):\n",
    "        x = np.asarray(x)\n",
    "        assert x.shape == self._M.shape\n",
    "        self._n += 1\n",
    "        if self._n == 1:\n",
    "            self._M[...] = x\n",
    "        else:\n",
    "            oldM = self._M.copy()\n",
    "            self._M[...] = oldM + (x - oldM) / self._n\n",
    "            self._S[...] = self._S + (x - oldM) * (x - self._M)\n",
    "\n",
    "    @property\n",
    "    def n(self):\n",
    "        return self._n\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self._M\n",
    "\n",
    "    @property\n",
    "    def var(self):\n",
    "        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(self.var)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._M.shape\n",
    "\n",
    "\n",
    "class ZFilter:\n",
    "    \"\"\"\n",
    "    y = (x-mean)/std\n",
    "    using running estimates of mean,std\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n",
    "        self.demean = demean\n",
    "        self.destd = destd\n",
    "        self.clip = clip\n",
    "\n",
    "        self.rs = RunningStat(shape)\n",
    "\n",
    "    def __call__(self, x, update=True):\n",
    "        if update: self.rs.push(x)\n",
    "        if self.demean:\n",
    "            x = x - self.rs.mean\n",
    "        if self.destd:\n",
    "            x = x / (self.rs.std + 1e-8)\n",
    "        if self.clip:\n",
    "            x = np.clip(x, -self.clip, self.clip)\n",
    "        return x\n",
    "\n",
    "    def output_shape(self, input_space):\n",
    "        return input_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDqYISHHyQve"
   },
   "outputs": [],
   "source": [
    "# Here, you need to finish the first 5 tasks.\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, layer_norm=True):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        '''\n",
    "        Q1:\n",
    "        Initialize your networks\n",
    "        '''\n",
    "        self.actor_fc1 = \n",
    "        self.actor_fc2 = \n",
    "        self.actor_fc3 = \n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, num_outputs))\n",
    "\n",
    "        self.critic_fc1 = \n",
    "        self.critic_fc2 = \n",
    "        self.critic_fc3 = \n",
    "\n",
    "        if layer_norm:\n",
    "            self.layer_norm(self.actor_fc1, std=1.0)\n",
    "            self.layer_norm(self.actor_fc2, std=1.0)\n",
    "            self.layer_norm(self.actor_fc3, std=0.01)\n",
    "\n",
    "            self.layer_norm(self.critic_fc1, std=1.0)\n",
    "            self.layer_norm(self.critic_fc2, std=1.0)\n",
    "            self.layer_norm(self.critic_fc3, std=1.0)\n",
    "\n",
    "    @staticmethod\n",
    "    def layer_norm(layer, std=1.0, bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "\n",
    "    def forward(self, states):\n",
    "        \"\"\"\n",
    "        Q2.1:\n",
    "        run policy network (actor) as well as value network (critic)\n",
    "        :param states: a tensor represents states\n",
    "        :return: 3 Tensor2\n",
    "        your _forward_actor() function should return both the mean value of action and the log-standard deviation of the action\n",
    "        \"\"\"\n",
    "\n",
    "        action_mean, action_logstd = self._forward_actor(states)\n",
    "        critic_value = self._forward_critic(states)\n",
    "        return action_mean, action_logstd, critic_value\n",
    "\n",
    "    def _forward_actor(self, states):\n",
    "        '''\n",
    "        Q2.2:\n",
    "        build something like \n",
    "        x = activation (actor_fc(state))\n",
    "        the logstd output has already been provided\n",
    "        '''\n",
    "        action_mean = \n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        return action_mean, action_logstd\n",
    "\n",
    "    def _forward_critic(self, states):\n",
    "        '''\n",
    "        Q2.3:\n",
    "        build something like \n",
    "        x = activation (critic_fc(state))'''\n",
    "\n",
    "        critic_value = self.critic_fc3(x)\n",
    "        return critic_value\n",
    "\n",
    "    def select_action(self, action_mean, action_logstd, return_logproba=True):\n",
    "        \"\"\"\n",
    "        Q3.1:\n",
    "        given mean and std, sample an action from normal(mean, std)\n",
    "        also returns probability of the given chosen\n",
    "        \"\"\"\n",
    "        \n",
    "        return action, logproba\n",
    "\n",
    "    @staticmethod\n",
    "    def _normal_logproba(x, mean, logstd, std=None):\n",
    "        '''\n",
    "        Q3.2:\n",
    "        given a mean and logstd of a gaussian,\n",
    "        calculate the log-probability of a given x'''\n",
    "\n",
    "        return logproba.sum(1)\n",
    "\n",
    "    def get_logproba(self, states, actions):\n",
    "        \"\"\"\n",
    "        return probability of chosen the given actions under corresponding states of current network\n",
    "        :param states: Tensor\n",
    "        :param actions: Tensor\n",
    "        \"\"\"\n",
    "        action_mean, action_logstd = self._forward_actor(states)\n",
    "        action_mean = action_mean.cpu()\n",
    "        action_logstd = action_logstd.cpu()\n",
    "        logproba = self._normal_logproba(actions, action_mean, action_logstd)\n",
    "        return logproba\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self):\n",
    "        return Transition(*zip(*self.memory))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "env = gym.make(ENV_NAME)  \n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "network = ActorCritic(num_inputs, num_actions, layer_norm=args.layer_norm)\n",
    "network.train()\n",
    "def ppo(args):\n",
    "    env = gym.make(args.env_name)\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.shape[0]\n",
    "\n",
    "    env.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    #network = ActorCritic(num_inputs, num_actions, layer_norm=args.layer_norm)\n",
    "    optimizer = opt.Adam(network.parameters(), lr=args.lr)\n",
    "\n",
    "    running_state = ZFilter((num_inputs,), clip=5.0)\n",
    "\n",
    "    # record average 1-round cumulative reward in every episode\n",
    "    reward_record = []\n",
    "    global_steps = 0\n",
    "\n",
    "    lr_now = args.lr\n",
    "    clip_now = args.clip\n",
    "\n",
    "    for i_episode in range(args.num_episode):\n",
    "        # step1: perform current policy to collect trajectories\n",
    "        # this is an on-policy method!\n",
    "        memory = Memory()\n",
    "        num_steps = 0\n",
    "        reward_list = []\n",
    "        len_list = []\n",
    "        while num_steps < args.batch_size:\n",
    "            state = env.reset()\n",
    "            if args.state_norm:\n",
    "                state = running_state(state)\n",
    "            reward_sum = 0\n",
    "            for t in range(args.max_step_per_round):\n",
    "                action_mean, action_logstd, value = network(Tensor(state).unsqueeze(0))\n",
    "                action, logproba = network.select_action(action_mean, action_logstd)\n",
    "                action = action.cpu().data.numpy()[0]\n",
    "                logproba = logproba.cpu().data.numpy()[0]\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                reward_sum += reward\n",
    "                if args.state_norm:\n",
    "                    next_state = running_state(next_state)\n",
    "                mask = 0 if done else 1\n",
    "\n",
    "                memory.push(state, value, action, logproba, mask, next_state, reward)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            num_steps += (t + 1)\n",
    "            global_steps += (t + 1)\n",
    "            reward_list.append(reward_sum)\n",
    "            len_list.append(t + 1)\n",
    "        reward_record.append({\n",
    "            'episode': i_episode, \n",
    "            'steps': global_steps, \n",
    "            'meanepreward': np.mean(reward_list), \n",
    "            'meaneplen': np.mean(len_list)})\n",
    "        rwds.extend(reward_list)\n",
    "        batch = memory.sample()\n",
    "        batch_size = len(memory)\n",
    "\n",
    "        # step2: extract variables from trajectories\n",
    "        rewards = Tensor(batch.reward)\n",
    "        values = Tensor(batch.value)\n",
    "        masks = Tensor(batch.mask)\n",
    "        actions = Tensor(batch.action)\n",
    "        states = Tensor(batch.state)\n",
    "        oldlogproba = Tensor(batch.logproba)\n",
    "\n",
    "        returns = Tensor(batch_size)\n",
    "        deltas = Tensor(batch_size)\n",
    "        advantages = Tensor(batch_size)\n",
    "\n",
    "        prev_return = 0\n",
    "        prev_value = 0\n",
    "        prev_advantage = 0\n",
    "        for i in reversed(range(batch_size)):\n",
    "            returns[i] = rewards[i] + args.gamma * prev_return * masks[i]\n",
    "            deltas[i] = rewards[i] + args.gamma * prev_value * masks[i] - values[i]\n",
    "            # ref: https://arxiv.org/pdf/1506.02438.pdf (generalization advantage estimate)\n",
    "            advantages[i] = deltas[i] + args.gamma * args.lamda * prev_advantage * masks[i]\n",
    "\n",
    "            prev_return = returns[i]\n",
    "            prev_value = values[i]\n",
    "            prev_advantage = advantages[i]\n",
    "        if args.advantage_norm:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + EPS)\n",
    "\n",
    "        for i_epoch in range(int(args.num_epoch * batch_size / args.minibatch_size)):\n",
    "            # sample from current batch\n",
    "            minibatch_ind = np.random.choice(batch_size, args.minibatch_size, replace=False)\n",
    "            minibatch_states = states[minibatch_ind]\n",
    "            minibatch_actions = actions[minibatch_ind]\n",
    "            minibatch_oldlogproba = oldlogproba[minibatch_ind]\n",
    "            minibatch_newlogproba = network.get_logproba(minibatch_states, minibatch_actions)\n",
    "            minibatch_advantages = advantages[minibatch_ind]\n",
    "            minibatch_returns = returns[minibatch_ind]\n",
    "            minibatch_newvalues = network._forward_critic(minibatch_states).flatten()\n",
    "\n",
    "\n",
    "\n",
    "            '''\n",
    "            Q4: \n",
    "\n",
    "            HERE: \n",
    "            now you have the advantages, and log-probabilities (both pi_new and pi_old)\n",
    "            you need to do optimization according to the CLIP loss\n",
    "            \n",
    "            '''\n",
    "            \n",
    "\n",
    "            if args.lossvalue_norm:\n",
    "                minibatch_return_6std = 6 * minibatch_returns.std()\n",
    "                loss_value = torch.mean((minibatch_newvalues - minibatch_returns).pow(2)) / minibatch_return_6std\n",
    "            else:\n",
    "                loss_value = torch.mean((minibatch_newvalues - minibatch_returns).pow(2))\n",
    "\n",
    "            loss_entropy = torch.mean(torch.exp(minibatch_newlogproba) * minibatch_newlogproba)\n",
    "\n",
    "            total_loss = loss_surr + args.loss_coeff_value * loss_value + args.loss_coeff_entropy * loss_entropy\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.schedule_clip == 'linear':\n",
    "            ep_ratio = 1 - (i_episode / args.num_episode)\n",
    "            clip_now = args.clip * ep_ratio\n",
    "\n",
    "        if args.schedule_adam == 'linear':\n",
    "            ep_ratio = 1 - (i_episode / args.num_episode)\n",
    "            lr_now = args.lr * ep_ratio\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = lr_now\n",
    "\n",
    "        if i_episode % args.log_num_episode == 0:\n",
    "            print('Finished episode: {} Reward: {:.4f} total_loss = {:.4f} = {:.4f} + {} * {:.4f} + {} * {:.4f}' \\\n",
    "                .format(i_episode, reward_record[-1]['meanepreward'], total_loss.data, loss_surr.data, args.loss_coeff_value, \n",
    "                loss_value.data, args.loss_coeff_entropy, loss_entropy.data))\n",
    "            print('-----------------')\n",
    "\n",
    "    return reward_record\n",
    "\n",
    "def test(args):\n",
    "    record_dfs = []\n",
    "    for i in range(args.num_parallel_run):\n",
    "        args.seed += 1\n",
    "        reward_record = pd.DataFrame(ppo(args))\n",
    "        reward_record['#parallel_run'] = i\n",
    "        record_dfs.append(reward_record)\n",
    "    record_dfs = pd.concat(record_dfs, axis=0)\n",
    "    record_dfs.to_csv(joindir(RESULT_DIR, 'ppo-record-{}.csv'.format(args.env_name)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for envname in [ENV_NAME]:\n",
    "        args.env_name = envname\n",
    "        test(args)\n",
    "\n",
    "torch.save(network.state_dict(),args.env_name.split('-')[0]+'/CheckPoints/checkpoint_new_{0}hidden_{1}drop_prob_{2}repeat'.format(args.hid_num,config.drop_prob,repeat)) \n",
    "np.savetxt(args.env_name.split('-')[0]+'/Rwds/rwds_new_{0}hidden_{1}drop_prob_{2}repeat'.format(args.hid_num,config.drop_prob,repeat),rwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVV1-fJWArMt"
   },
   "source": [
    "# DDPG and TD3\n",
    "\n",
    "The Deterministic Policy Gradient method was proposed by Silver et. al. 2014 (http://proceedings.mlr.press/v32/silver14.pdf), and DDPG is its deep version.\n",
    "\n",
    "The DPG also uses the actor-critic paradigm, but maitains a deterministic version of policy. It optimizes the critic through the Bellman Equation, and optimize the actor through the chain rule. \n",
    "\n",
    "In this assignment, you may need to import some python files like DDPG.py and TD3.py to insert the method into training.\n",
    "Here are some solutions from stackoverflow: https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab.\n",
    "\n",
    "It is easier to just copy it from Drive than upload it.\n",
    "1. Store MYLIB.py in your Drive. (for this assignment, it will be the utils.py, DDPG.py and TD3.py)\n",
    "2. Open the Colab.\n",
    "3. Open the left side pane, select Files view (the file icon).\n",
    "4. Click Mount Drive then Connect to Google Drive (the folder with google drive icon).\n",
    "5. Copy it by running \"! cp drive/My\\ Drive/MYLIB.py . \" in your Colab file code line.\n",
    "6. import MYLIB\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6OJdsG_MJz_"
   },
   "source": [
    "## TODOs for You (Please write down the answer in this block)\n",
    "\n",
    "The TD3 is short for *Twin Delayed Deep Deterministic Policy Gradient*, their official open-source implementation is extremely clear and easy to follow! So I believe there is no need for you to build up the wheels one more time.\n",
    "\n",
    "However, you really need to know about how this method works!\n",
    "TD3 proposes several improvements based on the method of DDPG to improve its sample efficiency.\n",
    "\n",
    "- Q6. In this part, your task is to read the paper, and read the code of the official implementation of TD3 and DDPG at:\n",
    "\n",
    "https://github.com/sfujim/TD3/blob/master/DDPG.py\n",
    "\n",
    "https://github.com/sfujim/TD3/blob/master/TD3.py\n",
    "\n",
    "Then, please try to find the proposed improvements in TD3 over DDPG and summary them HERE:\n",
    "\n",
    "1. \n",
    "    - code:\n",
    "2. \n",
    "    - code:\n",
    "3. \n",
    "    - code:\n",
    "4. \n",
    "    - code:\n",
    "\n",
    "- Q7. Among all those improvements, which do you believe is the most important one? You may take some ablation studies to support your claim.  (i.e., draw some learning curves with different settings together and draw your conclusions)\n",
    "\n",
    "- Q8. What is the difference between TD3(DDPG) and PPO in the OPTIMIZATION step (including but not restricted in terms of the sampling-training proportion)? Actually the improvements of PPO over TRPO was pointed  as a benefit of more training iterations, can you further improve the sample efficiency of TD3?\n",
    "\n",
    "- Q9. (i) Please describe the difference of the exploration strategies between PPO, DDPG and TD3. (ii) Provide a comparison between the exploration strategies of those continuous control algorithms and DQN.\n",
    "\n",
    "- Q10. (Bonus, 20 points) An open question. Do you think an epsilon-greedy-like exploration strategy you used in DQN/Q-learning is useful for continuous control? Will there be any problem of applying epsilon-greedy method in DDPG/TD3/PPO? Try to implement the idea and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following four blocks download the code in official implementation to your google drive so that the following script can run them. Note that the downloaded files may disappear due to some colab mechansim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "R2jXxeaWoCPM",
    "outputId": "31d18c26-8186-49ba-bcfc-ee72300fed50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TD3'...\n",
      "remote: Enumerating objects: 223, done.\u001b[K\n",
      "remote: Total 223 (delta 0), reused 0 (delta 0), pack-reused 223\u001b[K\n",
      "Receiving objects: 100% (223/223), 197.79 KiB | 691.00 KiB/s, done.\n",
      "Resolving deltas: 100% (62/62), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sfujim/TD3.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "66Sy-dOQ8Qo5"
   },
   "outputs": [],
   "source": [
    "!cp TD3/DDPG.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5TuZqwcHottp"
   },
   "outputs": [],
   "source": [
    "!cp TD3/TD3.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "259dqcCpotrJ"
   },
   "outputs": [],
   "source": [
    "!cp TD3/utils.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2X3lvH_5pKkS"
   },
   "outputs": [],
   "source": [
    "from os import makedirs as mkdir\n",
    "mkdir('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lboiuk9vHDhD"
   },
   "outputs": [],
   "source": [
    "# The following scripts run the DDPG algorithm.\n",
    "\n",
    "alias = 'ddpg' # an alias of your experiment, used as a label\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import TD3\n",
    "import DDPG\n",
    "\n",
    "def eval_policy(policy, eval_episodes=10):\n",
    "    eval_env = gym.make(ENV_NAME)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done,_ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "    #print(\"---------------------------------------\")\n",
    "    #print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    #print(\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "args_policy_noise = 0.2\n",
    "args_noise_clip = 0.5\n",
    "args_policy_freq = 2\n",
    "args_max_timesteps = 100000\n",
    "args_expl_noise = 0.1\n",
    "args_batch_size = 25\n",
    "args_eval_freq = 1000\n",
    "args_start_timesteps = 0\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": 0.99,\n",
    "    \"tau\": 0.005\n",
    "}\n",
    "\n",
    "\n",
    "args_policy = 'DDPG'\n",
    "\n",
    "if args_policy == \"TD3\":\n",
    "    # Target policy smoothing is scaled wrt the action scale\n",
    "    kwargs[\"policy_noise\"] = args_policy_noise * max_action\n",
    "    kwargs[\"noise_clip\"] = args_noise_clip * max_action\n",
    "    kwargs[\"policy_freq\"] = args_policy_freq\n",
    "    policy = TD3.TD3(**kwargs)\n",
    "elif args_policy == \"DDPG\":\n",
    "    policy = DDPG.DDPG(**kwargs)\n",
    "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "# Evaluate untrained policy\n",
    "evaluations = [eval_policy(policy)]\n",
    "\n",
    "state, done = env.reset(), False\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "counter = 0\n",
    "msk_list = []        \n",
    "temp_curve = [eval_policy(policy)]\n",
    "temp_val = []\n",
    "for t in range(int(args_max_timesteps)):\n",
    "    episode_timesteps += 1\n",
    "    counter += 1\n",
    "    # Select action randomly or according to policy\n",
    "    if t < args_start_timesteps:\n",
    "        action = np.random.uniform(-max_action,max_action,action_dim)\n",
    "    else:\n",
    "        if np.random.uniform(0,1) < 0.1:\n",
    "            action = np.random.uniform(-max_action,max_action,action_dim)\n",
    "        else:\n",
    "            action = (\n",
    "                policy.select_action(np.array(state))\n",
    "                + np.random.normal(0, max_action * args_expl_noise, size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "    # Perform action\n",
    "    next_state, reward, done,_ = env.step(action) \n",
    "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    if t >= args_start_timesteps:\n",
    "        '''TD3'''\n",
    "        last_val = 999.\n",
    "        patient = 5\n",
    "        for i in range(1):\n",
    "            policy.train(replay_buffer, args_batch_size)\n",
    "                \n",
    "\n",
    "    # Train agent after collecting sufficient data\n",
    "    if done: \n",
    "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "        msk_list = []\n",
    "        state, done = env.reset(), False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1 \n",
    "\n",
    "    # Evaluate episode\n",
    "    if (t + 1) % args_eval_freq == 0:\n",
    "        evaluations.append(eval_policy(policy))\n",
    "        print('recent Evaluation:',evaluations[-1])\n",
    "        np.save('results/evaluations_alias{}_ENV{}'.format(alias,ENV_NAME),evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAVa6S3yG5m5"
   },
   "outputs": [],
   "source": [
    "# The following scripts run the TD3 algorithm.\n",
    "\n",
    "alias = 'td3'\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import argparse\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "import TD3\n",
    "import DDPG\n",
    "\n",
    "def eval_policy(policy, eval_episodes=10):\n",
    "    eval_env = gym.make(ENV_NAME)\n",
    "\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, done,_ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "    #print(\"---------------------------------------\")\n",
    "    #print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    #print(\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = env.action_space.high[0]\n",
    "\n",
    "args_policy_noise = 0.2\n",
    "args_noise_clip = 0.5\n",
    "args_policy_freq = 2\n",
    "args_max_timesteps = 100000\n",
    "args_expl_noise = 0.1\n",
    "args_batch_size = 25\n",
    "args_eval_freq = 1000\n",
    "args_start_timesteps = 0\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\": state_dim,\n",
    "    \"action_dim\": action_dim,\n",
    "    \"max_action\": max_action,\n",
    "    \"discount\": 0.99,\n",
    "    \"tau\": 0.005\n",
    "}\n",
    "\n",
    "\n",
    "args_policy = 'TD3'\n",
    "\n",
    "if args_policy == \"TD3\":\n",
    "    # Target policy smoothing is scaled wrt the action scale\n",
    "    kwargs[\"policy_noise\"] = args_policy_noise * max_action\n",
    "    kwargs[\"noise_clip\"] = args_noise_clip * max_action\n",
    "    kwargs[\"policy_freq\"] = args_policy_freq\n",
    "    policy = TD3.TD3(**kwargs)\n",
    "elif args_policy == \"OurDDPG\":\n",
    "    policy = OurDDPG.DDPG(**kwargs)\n",
    "elif args_policy == \"DDPG\":\n",
    "    policy = DDPG.DDPG(**kwargs)\n",
    "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "# Evaluate untrained policy\n",
    "evaluations = [eval_policy(policy)]\n",
    "\n",
    "state, done = env.reset(), False\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "counter = 0\n",
    "msk_list = []        \n",
    "temp_curve = [eval_policy(policy)]\n",
    "temp_val = []\n",
    "for t in range(int(args_max_timesteps)):\n",
    "    episode_timesteps += 1\n",
    "    counter += 1\n",
    "    # Select action randomly or according to policy\n",
    "    if t < args_start_timesteps:\n",
    "        action = np.random.uniform(-max_action,max_action,action_dim)\n",
    "    else:\n",
    "        if np.random.uniform(0,1) < 0.1:\n",
    "            action = np.random.uniform(-max_action,max_action,action_dim)\n",
    "        else:\n",
    "            action = (\n",
    "                policy.select_action(np.array(state))\n",
    "                + np.random.normal(0, max_action * args_expl_noise, size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "    # Perform action\n",
    "    next_state, reward, done,_ = env.step(action)\n",
    "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    if t >= args_start_timesteps:\n",
    "        '''TD3'''\n",
    "        last_val = 999.\n",
    "        patient = 5\n",
    "        for i in range(1):\n",
    "            policy.train(replay_buffer, args_batch_size)\n",
    "                \n",
    "\n",
    "    # Train agent after collecting sufficient data\n",
    "    if done: \n",
    "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "        msk_list = []\n",
    "        state, done = env.reset(), False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1 \n",
    "\n",
    "    # Evaluate episode\n",
    "    if (t + 1) % args_eval_freq == 0:\n",
    "        evaluations.append(eval_policy(policy))\n",
    "        print('recent Evaluation:',evaluations[-1])\n",
    "        np.save('results/evaluations_alias{}_ENV{}'.format(alias,ENV_NAME),evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KboX9hB5eEVv"
   },
   "source": [
    "# Four-Solution-Maze Environment (optional)\n",
    "\n",
    "## TODOs for you:\n",
    "\n",
    "- Q11. (bonus) In this section, another environment named Four-Solution-Maze is provided for you to evaluate your algorithms.\n",
    "\n",
    "The task is quite simple, yet never easy for even PPO/TD3.\n",
    "\n",
    "The default size of the maze is 64x64, and in each game (espisode), the agent is initialized randomly in the maze. There are 4 positions in the maze that has non-trivial reward of +10, while reaching other region will recieve only a tiny punishment of -0.1. An optimal policy should be able to find the shortest path to the most recent reward region (i.e., one of the four high-reward regions.).\n",
    "\n",
    "The action space is continuous with range [-1,1], larger actions will be clipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kX6V-0bfMSev"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "from numpy import *\n",
    "import copy\n",
    "\n",
    "class FourWayGridWorld:\n",
    "    def __init__(self, N=17,left = 10,right = 10, up=10, down = 10):\n",
    "        self.N = N\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "        self.map = np.ones((N,N))*(-0.1)\n",
    "        self.map[int((N-1)/2),0] = self.left\n",
    "        self.map[0,int((N-1)/2)] = self.up\n",
    "        self.map[N-1,int((N-1)/2)] = self.down\n",
    "        self.map[int((N-1)/2),N-1] = self.right\n",
    "        self.loc = np.asarray([np.random.randint(N),np.random.randint(N)])\n",
    "        self.step_num = 0\n",
    "    def step(self,action):\n",
    "        action = np.clip(action,-1,1)\n",
    "        new_loc = np.clip(self.loc + action,0,self.N-1)\n",
    "        self.loc = new_loc\n",
    "        reward = self.map[int(round(self.loc[0])),int(round(self.loc[1]))]\n",
    "        self.step_num+=1\n",
    "        return self.loc,reward,self.ifdone()\n",
    "    def ifdone(self):\n",
    "        if self.step_num >= 2*self.N:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def render(self):\n",
    "        map_self = copy.deepcopy(self.map)\n",
    "        map_self[int(self.loc[0]),int(self.loc[1])] = -5\n",
    "        plt.imshow(map_self)\n",
    "    def reset(self):\n",
    "        self.map = np.ones((self.N,self.N))*(-0.1)\n",
    "        self.map[int((self.N-1)/2),0] = self.left\n",
    "        self.map[0,int((self.N-1)/2)] = self.up\n",
    "        self.map[self.N-1,int((self.N-1)/2)] = self.down\n",
    "        self.map[int((self.N-1)/2),self.N-1] = self.right\n",
    "        self.loc = np.asarray([np.random.randint(self.N),np.random.randint(self.N)])\n",
    "        self.step_num = 0\n",
    "        return self.loc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoO_Ti-zhykw"
   },
   "outputs": [],
   "source": [
    "env = FourWayGridWorld(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rBzb2qw3h2ts",
    "outputId": "458ee588-3f5b-4f19-d45c-952b6b330a73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30,  2])"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "SBcwx2WPh3nT",
    "outputId": "1b60ea64-3859-4feb-8bbf-db6e72ca287e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WrHjDQmqC7K"
   },
   "outputs": [],
   "source": [
    "# This section is used to visualize your learned policy\n",
    "from torch import Tensor\n",
    "output_i = np.zeros((33,33))\n",
    "output_j = np.zeros((33,33))\n",
    "output_i_m = np.zeros((33,33))\n",
    "output_j_m = np.zeros((33,33))\n",
    "value_ij = np.zeros((33,33))\n",
    "for i in range(33):\n",
    "    for j in range(33):\n",
    "        states = Tensor(np.asarray([i,j])).float().unsqueeze(0)\n",
    "        \n",
    "        '''\n",
    "        you need to revise the following line, \n",
    "        to fit your policy/network outputs\n",
    "        '''\n",
    "        action, value = policy(states)\n",
    "        output_i[i,j] = action[0]\n",
    "        output_j[i,j] = action[1]\n",
    "        value_ij[i,j] = value\n",
    "        \n",
    "plt.figure(figsize= (5,5))\n",
    "for i in range(33):\n",
    "    for j in range(33):\n",
    "        plt.arrow(j,-i,output_j[i,j],-output_i[i,j],head_width=0.2,shape='left')\n",
    "xlim(-1,33)\n",
    "ylim(-33,1)\n",
    "yticks([2*i-32 for i in range(17)],[2*i for i in range(17)])\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5YEdvVdqDd7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment4_W/O_Sol.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
