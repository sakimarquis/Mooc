{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "import numpy.testing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3.blocking import DictToArrayBijection, ArrayOrdering\n",
    "\n",
    "\n",
    "def build_joined(cost, args, ordering):\n",
    "    vmap = ordering.vmap\n",
    "    size = ordering.size\n",
    "    dtype = theano.config.floatX\n",
    "    args_joined = tt.vector('__args_joined')\n",
    "    args_joined.tag.test_value = np.zeros(size, dtype=dtype)\n",
    "\n",
    "    joined_slices = {}\n",
    "    for vmap in vmap:\n",
    "        sliced = args_joined[vmap.slc].reshape(vmap.shp)\n",
    "        sliced.name = vmap.var\n",
    "        joined_slices[vmap.var] = sliced\n",
    "\n",
    "    replace = {var: joined_slices[var.name] for var in args}\n",
    "    return args_joined, theano.clone(cost, replace=replace)\n",
    "\n",
    "\n",
    "def logp_func(cost, grad_vars, **kwargs):\n",
    "    ordering = ArrayOrdering(grad_vars)\n",
    "    vars_joined, cost_joined = build_joined(\n",
    "        cost, grad_vars, ordering)\n",
    "\n",
    "    grad = tt.grad(cost_joined, vars_joined)\n",
    "    grad.name = '__grad'\n",
    "\n",
    "    inputs = [vars_joined]\n",
    "\n",
    "    return theano.function(inputs, [cost_joined, grad], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [nu_log__, sigma_log__]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 203.55it/s]\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '13530' (I am process '13531')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/laoj/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '13530' (I am process '13532')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/laoj/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '13531' (I am process '13532')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/laoj/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir\n"
     ]
    }
   ],
   "source": [
    "# Edge case discovered in #2948\n",
    "ndim = 3\n",
    "with pm.Model() as m:\n",
    "    pm.Lognormal('sigma',\n",
    "                 mu=np.zeros(ndim),\n",
    "                 tau=np.ones(ndim),\n",
    "                 shape=ndim)  # variance for the correlation matrix\n",
    "    pm.HalfCauchy('nu', beta=10)\n",
    "    tr = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [var.logpt for var in m.basic_RVs] + m.potentials \n",
    "logpt_map = tt.add(*map(tt.sum, factors))\n",
    "logpt_sum = tt.sum([tt.sum(factor) for factor in factors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function profiling\n",
      "==================\n",
      "  Message: /home/laoj/Documents/Github/pymc3/pymc3/model.py:902\n",
      "  Time in 1000 calls to Function.__call__: 1.322675e-02s\n",
      "  Time in Function.fn.__call__: 1.751661e-03s (13.243%)\n",
      "  Time in thunks: 5.264282e-04s (3.980%)\n",
      "  Total compile time: 1.107237e-01s\n",
      "    Number of Apply nodes: 3\n",
      "    Theano Optimizer time: 5.706906e-02s\n",
      "       Theano validate time: 4.184246e-04s\n",
      "    Theano Linker time (includes C, CUDA code generation/compiling): 1.852036e-03s\n",
      "       Import time 0.000000e+00s\n",
      "       Node make_thunk time 1.621962e-03s\n",
      "           Node Elemwise{Composite{(i0 + Switch(Cast{int8}(GE(exp(i1), i2)), (i3 - log1p(sqr((i4 * exp(i1))))), i5) + i1)}}[(0, 0)](__logp_sigma_log__, nu_log__, TensorConstant{0}, TensorConstant{-2.7541678283542828}, TensorConstant{0.1}, TensorConstant{-inf}) time 6.444454e-04s\n",
      "           Node Elemwise{Composite{((i0 + (i1 * sqr(Composite{log(exp(i0))}(i2))) + i2) - Composite{log(exp(i0))}(i2))}}(TensorConstant{(1,) of -0..5332046727}, TensorConstant{(1,) of -0.5}, sigma_log__) time 6.155968e-04s\n",
      "           Node Sum{acc_dtype=float64}(Elemwise{Composite{((i0 + (i1 * sqr(Composite{log(exp(i0))}(i2))) + i2) - Composite{log(exp(i0))}(i2))}}.0) time 3.581047e-04s\n",
      "\n",
      "Time in all call to theano.grad() 1.481418e+00s\n",
      "Time since theano import 115.923s\n",
      "Class\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>\n",
      "  83.8%    83.8%       0.000s       2.21e-07s     C     2000       2   theano.tensor.elemwise.Elemwise\n",
      "  16.2%   100.0%       0.000s       8.51e-08s     C     1000       1   theano.tensor.elemwise.Sum\n",
      "   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Ops\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>\n",
      "  54.2%    54.2%       0.000s       2.85e-07s     C     1000        1   Elemwise{Composite{((i0 + (i1 * sqr(Composite{log(exp(i0))}(i2))) + i2) - Composite{log(exp(i0))}(i2))}}\n",
      "  29.6%    83.8%       0.000s       1.56e-07s     C     1000        1   Elemwise{Composite{(i0 + Switch(Cast{int8}(GE(exp(i1), i2)), (i3 - log1p(sqr((i4 * exp(i1))))), i5) + i1)}}[(0, 0)]\n",
      "  16.2%   100.0%       0.000s       8.51e-08s     C     1000        1   Sum{acc_dtype=float64}\n",
      "   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Apply\n",
      "------\n",
      "<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>\n",
      "  54.2%    54.2%       0.000s       2.85e-07s   1000     0   Elemwise{Composite{((i0 + (i1 * sqr(Composite{log(exp(i0))}(i2))) + i2) - Composite{log(exp(i0))}(i2))}}(TensorConstant{(1,) of -0..5332046727}, TensorConstant{(1,) of -0.5}, sigma_log__)\n",
      "  29.6%    83.8%       0.000s       1.56e-07s   1000     2   Elemwise{Composite{(i0 + Switch(Cast{int8}(GE(exp(i1), i2)), (i3 - log1p(sqr((i4 * exp(i1))))), i5) + i1)}}[(0, 0)](__logp_sigma_log__, nu_log__, TensorConstant{0}, TensorConstant{-2.7541678283542828}, TensorConstant{0.1}, TensorConstant{-inf})\n",
      "  16.2%   100.0%       0.000s       8.51e-08s   1000     1   Sum{acc_dtype=float64}(Elemwise{Composite{((i0 + (i1 * sqr(Composite{log(exp(i0))}(i2))) + i2) - Composite{log(exp(i0))}(i2))}}.0)\n",
      "   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)\n",
      "\n",
      "Here are tips to potentially make your code run faster\n",
      "                 (if you think of new ones, suggest them on the mailing list).\n",
      "                 Test them first, as they are not guaranteed to always provide a speedup.\n",
      "  - Try the Theano flag floatX=float32\n",
      "  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.\n"
     ]
    }
   ],
   "source": [
    "m.profile(logpt_map).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function profiling\n",
      "==================\n",
      "  Message: /home/laoj/Documents/Github/pymc3/pymc3/model.py:902\n",
      "  Time in 1000 calls to Function.__call__: 1.394749e-02s\n",
      "  Time in Function.fn.__call__: 2.378464e-03s (17.053%)\n",
      "  Time in thunks: 7.913113e-04s (5.674%)\n",
      "  Total compile time: 1.033812e-01s\n",
      "    Number of Apply nodes: 5\n",
      "    Theano Optimizer time: 5.309653e-02s\n",
      "       Theano validate time: 3.561974e-04s\n",
      "    Theano Linker time (includes C, CUDA code generation/compiling): 2.921343e-03s\n",
      "       Import time 0.000000e+00s\n",
      "       Node make_thunk time 2.716064e-03s\n",
      "           Node Elemwise{Composite{(Switch(Cast{int8}(GE(exp(i0), i1)), (i2 - log1p(sqr((i3 * exp(i0))))), i4) + i0)}}(nu_log__, TensorConstant{0}, TensorConstant{-2.7541678283542828}, TensorConstant{0.1}, TensorConstant{-inf}) time 1.189947e-03s\n",
      "           Node Elemwise{Composite{((i0 + (i1 * sqr(Composite{log(exp(i0))}(i2))) + i2) - Composite{log(exp(i0))}(i2))}}(TensorConstant{(1,) of -0..5332046727}, TensorConstant{(1,) of -0.5}, sigma_log__) time 6.387234e-04s\n",
      "           Node Sum{acc_dtype=float64}(MakeVector{dtype='float64'}.0) time 3.104210e-04s\n",
      "           Node MakeVector{dtype='float64'}(__logp_sigma_log__, __logp_nu_log__) time 2.915859e-04s\n",
      "           Node Sum{acc_dtype=float64}(Elemwise{Composite{((i0 + (i1 * sqr(Composite{log(exp(i0))}(i2))) + i2) - Composite{log(exp(i0))}(i2))}}.0) time 2.799034e-04s\n",
      "\n",
      "Time in all call to theano.grad() 1.481418e+00s\n",
      "Time since theano import 132.909s\n",
      "Class\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>\n",
      "  62.2%    62.2%       0.000s       2.46e-07s     C     2000       2   theano.tensor.elemwise.Elemwise\n",
      "  21.5%    83.7%       0.000s       8.51e-08s     C     2000       2   theano.tensor.elemwise.Sum\n",
      "  16.3%   100.0%       0.000s       1.29e-07s     C     1000       1   theano.tensor.opt.MakeVector\n",
      "   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Ops\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>\n",
      "  35.9%    35.9%       0.000s       2.84e-07s     C     1000        1   Elemwise{Composite{((i0 + (i1 * sqr(Composite{log(exp(i0))}(i2))) + i2) - Composite{log(exp(i0))}(i2))}}\n",
      "  26.3%    62.2%       0.000s       2.08e-07s     C     1000        1   Elemwise{Composite{(Switch(Cast{int8}(GE(exp(i0), i1)), (i2 - log1p(sqr((i3 * exp(i0))))), i4) + i0)}}\n",
      "  21.5%    83.7%       0.000s       8.51e-08s     C     2000        2   Sum{acc_dtype=float64}\n",
      "  16.3%   100.0%       0.000s       1.29e-07s     C     1000        1   MakeVector{dtype='float64'}\n",
      "   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Apply\n",
      "------\n",
      "<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>\n",
      "  35.9%    35.9%       0.000s       2.84e-07s   1000     1   Elemwise{Composite{((i0 + (i1 * sqr(Composite{log(exp(i0))}(i2))) + i2) - Composite{log(exp(i0))}(i2))}}(TensorConstant{(1,) of -0..5332046727}, TensorConstant{(1,) of -0.5}, sigma_log__)\n",
      "  26.3%    62.2%       0.000s       2.08e-07s   1000     0   Elemwise{Composite{(Switch(Cast{int8}(GE(exp(i0), i1)), (i2 - log1p(sqr((i3 * exp(i0))))), i4) + i0)}}(nu_log__, TensorConstant{0}, TensorConstant{-2.7541678283542828}, TensorConstant{0.1}, TensorConstant{-inf})\n",
      "  16.3%    78.5%       0.000s       1.29e-07s   1000     3   MakeVector{dtype='float64'}(__logp_sigma_log__, __logp_nu_log__)\n",
      "  12.1%    90.6%       0.000s       9.61e-08s   1000     2   Sum{acc_dtype=float64}(Elemwise{Composite{((i0 + (i1 * sqr(Composite{log(exp(i0))}(i2))) + i2) - Composite{log(exp(i0))}(i2))}}.0)\n",
      "   9.4%   100.0%       0.000s       7.41e-08s   1000     4   Sum{acc_dtype=float64}(MakeVector{dtype='float64'}.0)\n",
      "   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)\n",
      "\n",
      "Here are tips to potentially make your code run faster\n",
      "                 (if you think of new ones, suggest them on the mailing list).\n",
      "                 Test them first, as they are not guaranteed to always provide a speedup.\n",
      "  - Try the Theano flag floatX=float32\n",
      "  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.\n"
     ]
    }
   ],
   "source": [
    "m.profile(logpt_sum).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_map = theano.function(m.basic_RVs, logpt_map)\n",
    "func_sum = theano.function(m.basic_RVs, logpt_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_map2 = logp_func(logpt_map, m.basic_RVs)\n",
    "func_sum2 = logp_func(logpt_sum, m.basic_RVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at map.png\n",
      "The output file is available at func_map.png\n",
      "The output file is available at func_map2.png\n"
     ]
    }
   ],
   "source": [
    "theano.printing.pydotprint(logpt_map, outfile=\"map.png\", var_with_name_simple=True)\n",
    "theano.printing.pydotprint(func_map, outfile=\"func_map.png\", var_with_name_simple=True)\n",
    "theano.printing.pydotprint(func_map2, outfile=\"func_map2.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unopt](map.png)\n",
    "![opt](func_map.png)\n",
    "![opt2](func_map2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at sum.png\n",
      "The output file is available at func_sum.png\n",
      "The output file is available at func_sum2.png\n"
     ]
    }
   ],
   "source": [
    "theano.printing.pydotprint(logpt_sum, outfile=\"sum.png\", var_with_name_simple=True)\n",
    "theano.printing.pydotprint(func_sum, outfile=\"func_sum.png\", var_with_name_simple=True)\n",
    "theano.printing.pydotprint(func_sum2, outfile=\"func_sum2.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unopt](sum.png)\n",
    "![opt](func_sum.png)\n",
    "![opt2](func_sum2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_map2_ = logp_func(logpt_map, m.basic_RVs, profile=True)\n",
    "func_sum2_ = logp_func(logpt_sum, m.basic_RVs, profile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function profiling\n",
      "==================\n",
      "  Message: <ipython-input-2-78031b7e3b98>:31\n",
      "  Time in 0 calls to Function.__call__: 0.000000e+00s\n",
      "  Total compile time: 2.107468e-01s\n",
      "    Number of Apply nodes: 19\n",
      "    Theano Optimizer time: 1.465194e-01s\n",
      "       Theano validate time: 2.043962e-03s\n",
      "    Theano Linker time (includes C, CUDA code generation/compiling): 1.086473e-02s\n",
      "       Import time 0.000000e+00s\n",
      "       Node make_thunk time 9.767771e-03s\n",
      "           Node Elemwise{Composite{(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(i0, i1), i2) - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(i3, i1), i2), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(i0, i1), i2)), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(i3, i1), i2), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(i0, i1), i2)))}}[(0, 1)](TensorConstant{4}, Shape_i{0}.0, TensorConstant{0}, TensorConstant{3}) time 7.956028e-04s\n",
      "           Node Shape_i{0}(__args_joined) time 7.114410e-04s\n",
      "           Node Elemwise{Composite{((i0 + (i1 * sqr(i2)) + i3) - i2)}}[(0, 2)](TensorConstant{(1,) of -0..5332046727}, TensorConstant{(1,) of -0.5}, Elemwise{Composite{log(exp(i0))}}.0, sigma_log__) time 6.840229e-04s\n",
      "           Node Elemwise{Composite{(i0 + Switch(i1, (i2 - log1p(i3)), i4) + i5)}}[(0, 0)](__logp_sigma_log__, Elemwise{Composite{Cast{int8}(GE(i0, i1))}}.0, TensorConstant{-2.7541678283542828}, Elemwise{Composite{sqr((i0 * i1))}}.0, TensorConstant{-inf}, nu_log__) time 6.656647e-04s\n",
      "           Node Elemwise{Composite{(i0 + Switch(i1, ((i2 * i3 * i3) / (i0 + i4)), i5))}}[(0, 3)](TensorConstant{1.0}, Elemwise{Composite{Cast{int8}(GE(i0, i1))}}.0, TensorConstant{-0.02}, Elemwise{exp,no_inplace}.0, Elemwise{Composite{sqr((i0 * i1))}}.0, TensorConstant{0}) time 6.251335e-04s\n",
      "\n",
      "Time in all call to theano.grad() 1.221900e+00s\n",
      "Time since theano import 32.835s\n",
      "Here are tips to potentially make your code run faster\n",
      "                 (if you think of new ones, suggest them on the mailing list).\n",
      "                 Test them first, as they are not guaranteed to always provide a speedup.\n",
      "  - Try the Theano flag floatX=float32\n"
     ]
    }
   ],
   "source": [
    "func_map2_.profile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function profiling\n",
      "==================\n",
      "  Message: <ipython-input-2-78031b7e3b98>:31\n",
      "  Time in 0 calls to Function.__call__: 0.000000e+00s\n",
      "  Total compile time: 2.195935e-01s\n",
      "    Number of Apply nodes: 21\n",
      "    Theano Optimizer time: 1.533418e-01s\n",
      "       Theano validate time: 2.004147e-03s\n",
      "    Theano Linker time (includes C, CUDA code generation/compiling): 1.212978e-02s\n",
      "       Import time 0.000000e+00s\n",
      "       Node make_thunk time 1.125216e-02s\n",
      "           Node Elemwise{Composite{Cast{int8}(GE(i0, i1))}}(Elemwise{exp,no_inplace}.0, TensorConstant{0}) time 2.016544e-03s\n",
      "           Node Elemwise{Composite{(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(i0, i1), i2) - Switch(LT(Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(i3, i1), i2), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(i0, i1), i2)), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(i3, i1), i2), Composite{Switch(LT(i0, i1), i1, i0)}(Composite{Switch(GE(i0, i1), i1, i0)}(i0, i1), i2)))}}[(0, 1)](TensorConstant{4}, Shape_i{0}.0, TensorConstant{0}, TensorConstant{3}) time 8.809566e-04s\n",
      "           Node Elemwise{Composite{((-i0) + i1 + i2)}}(Elemwise{Composite{log(exp(i0))}}.0, TensorConstant{(1,) of -1.0}, TensorConstant{(1,) of 1.0}) time 6.337166e-04s\n",
      "           Node Elemwise{Composite{(Switch(i0, ((i1 * i2 * i2) / (i3 + i4)), i5) + i6)}}[(0, 2)](Elemwise{Composite{Cast{int8}(GE(i0, i1))}}.0, TensorConstant{-0.02}, Elemwise{exp,no_inplace}.0, TensorConstant{1.0}, Elemwise{Composite{sqr((i0 * i1))}}.0, TensorConstant{0}, TensorConstant{1.0}) time 6.330013e-04s\n",
      "           Node Elemwise{Composite{((i0 + (i1 * sqr(i2)) + i3) - i2)}}[(0, 2)](TensorConstant{(1,) of -0..5332046727}, TensorConstant{(1,) of -0.5}, Elemwise{Composite{log(exp(i0))}}.0, sigma_log__) time 6.220341e-04s\n",
      "\n",
      "Time in all call to theano.grad() 1.221900e+00s\n",
      "Time since theano import 32.872s\n",
      "Here are tips to potentially make your code run faster\n",
      "                 (if you think of new ones, suggest them on the mailing list).\n",
      "                 Test them first, as they are not guaranteed to always provide a speedup.\n",
      "  - Try the Theano flag floatX=float32\n"
     ]
    }
   ],
   "source": [
    "func_sum2_.profile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_map3 = pm.model.ValueGradFunction(logpt_map, m.basic_RVs)\n",
    "func_sum3 = pm.model.ValueGradFunction(logpt_sum, m.basic_RVs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at func_map3.png\n",
      "The output file is available at func_sum3.png\n"
     ]
    }
   ],
   "source": [
    "theano.printing.pydotprint(func_map3._theano_function, outfile=\"func_map3.png\", var_with_name_simple=True)\n",
    "theano.printing.pydotprint(func_sum3._theano_function, outfile=\"func_sum3.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![opt3](func_map3.png)\n",
    "![opt3](func_sum3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model??"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Documents/Github/Planet_Sakaar_Data_Science/Miscellaneous/theano_graph_related.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
