{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Neural networks - Part C (20 points)\n",
    "## A neural network model of semantic cognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by *Brenden Lake* and *Todd Gureckis*  \n",
    "Computational Cognitive Modeling  \n",
    "NYU class webpage: https://brendenlake.github.io/CCM-site/  \n",
    "email to course instructors: instructors-ccm-spring2020@nyuccl.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "This homework is due before midnight on Monday Feb. 24, 2020.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will help implement and analyze a neural network model of semantic cognition. Semantic cognition is our intuitive understanding of objects and their properties. Semantic knowledge includes observations of which objects have which properties, and storage of these facts in long term memory. It also includes the ability to generalize, or predict which properties apply to which objects although they have not been directly observed.\n",
    "\n",
    "This notebook explores a neural network model of semantic cognition developed by Rogers and McClelland (R&M). R&M sought to model aspects of semantic cognition with a multi-layer neural network, which contrasts with classic symbolic approaches for organizing semantic knowledge. They model the cognitive development of semantic representation as gradient descent (the backpropgation algorithm), using a neural network trained to map objects to their corresponding properties. R&M also modeled the deterioration of semantic knowledge in dementia by adding noise to the learned representations.\n",
    "\n",
    "The network architecture is illustrated below.\n",
    "<img src=\"images/semcog_net.jpeg\" style=\"width: 450px;\"/>\n",
    "There are two input layers (\"Item Layer\" and \"Relation Layer\"), which pass through intermediate layers to produce an output pattern on the \"Attribute Layer.\" In this example, dark green is used to indicate active nodes (activation 1) and light green for inactive nodes (activation 0). The network is trained to answer queries involving an item (e.g., “Canary”) and a relation (e.g.,  “CAN”), outputting all attributes that are true of the item/relation pair (e.g., “grow, move, fly, sing”).\n",
    "\n",
    "For this assignment, you will set up the network architecture in PyTorch and train it. The dataset and code for training has been provided. You will then analyze how its semantic knowledge develops of the course of training. While the original model used logistic (sigmoid) activation functions for all of the intermediate and output layers, we will use the ReLu activation for the Representation and Hidden Layers, with a sigmoid activation for the Attribute Layer.\n",
    "\n",
    "Completing this assignment requires knowledge of setting up a neural network architecture in PyTorch. Please review your notes from lab, and these three basic [PyTorch tutorials](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html), \"What is PyTorch?\", \"Autograd\", and \"Neural Networks\" which should have the basics you need.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Reference (on NYU Classes):\n",
    "    \n",
    "McClelland, J. L., & Rogers, T. T. (2003). The parallel distributed processing approach to semantic cognition. Nature Reviews Neuroscience, 4(4), 310.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import print_function\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import sigmoid, relu\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load in the names of all the items, attributes, and relations into Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('data/sem_items.txt','r') as fid:\n",
    "    names_items = np.array([l.strip() for l in fid.readlines()])\n",
    "with open('data/sem_relations.txt','r') as fid:\n",
    "    names_relations = np.array([l.strip() for l in fid.readlines()])\n",
    "with open('data/sem_attributes.txt','r') as fid:\n",
    "    names_attributes = np.array([l.strip() for l in fid.readlines()])\n",
    "        \n",
    "nobj = len(names_items)\n",
    "nrel = len(names_relations)\n",
    "nattributes = len(names_attributes)\n",
    "print('List of items:')\n",
    "print(names_items)\n",
    "print(\"List of relations:\")\n",
    "print(names_relations)\n",
    "print(\"List of attributes:\")\n",
    "print(names_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load in the data matrix from a text file too. The matrix `D` has a row for each training pattern. It is split into a matrix of input patterns `input_pats` (item and relation) and their corresponding output patterns `output_pats` (attributes). The are `N` patterns total in the set.\n",
    "\n",
    "For each input pattern, the first 8 elements indicate which item is being presented, and the next 4 indicate which relation is being queried. Each element of the output pattern corresponds to a different attribute. All patterns use 1-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.loadtxt('data/sem_data.txt')\n",
    "input_pats = D[:,:nobj+nrel]\n",
    "input_pats = torch.tensor(input_pats,dtype=torch.float)\n",
    "output_pats = D[:,nobj+nrel:]\n",
    "output_pats = torch.tensor(output_pats,dtype=torch.float)\n",
    "N = input_pats.shape[0] # number of training patterns\n",
    "input_v = input_pats[0,:].numpy().astype('bool')\n",
    "output_v = output_pats[0,:].numpy().astype('bool')\n",
    "print('Example input pattern:')\n",
    "print(input_v.astype('int'))\n",
    "print('Example output pattern:')\n",
    "print(output_v.astype('int'))\n",
    "print(\"\")\n",
    "print(\"Which encodes...\")\n",
    "print('Item ',end='')\n",
    "print(names_items[input_v[:8]])\n",
    "print('Relation ',end='')\n",
    "print(names_relations[input_v[8:]])\n",
    "print('Attributes ',end='')\n",
    "print(names_attributes[output_v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 1 (15 points) </h3>\n",
    "<br>\n",
    "Your assignment is to create the neural network architecture shown in the figure above. Fill in the missing pieces of the \"Net\" class in the code below. For an example, refer to the PyTorch tutorial on <a href=\"https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\">\"Neural Networks\"</a>.\n",
    "<br><br>\n",
    "Use the ReLu activation function (\"relu\") for the Representation and Hidden Layers, with a Logistic/Sigmoid activation function for the Attribute Layer (\"sigmoid\").    \n",
    "<br><br>\n",
    "You will need PyTorch's \"nn.Linear\" function for constructing the layers, and the \"relu\" and \"sigmoid\" activation functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, rep_size, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "        # Input\n",
    "        #  rep_size : number of hidden units in \"Representation Layer\"\n",
    "        #  hidden_Size : number of hidden units in \"Hidden Layer\"\n",
    "        #\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        raise Exception('Replace with your code.')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Defines forward pass for the network on input patterns x\n",
    "        #\n",
    "        # Input can take these two forms:\n",
    "        #\n",
    "        #   x: [nobj+nrel 1D Tensor], which is a single input pattern as a 1D tensor\n",
    "        #      (containing both object and relation 1-hot identifier) (batch size is B=1)\n",
    "        #   OR\n",
    "        #   x : [B x (nobj+nrel) Tensor], which is a batch of B input patterns (one for each row)\n",
    "        #\n",
    "        # Output\n",
    "        #   output [B x nattribute Tensor], which is the output pattern for each input pattern B on the Attribute Layer\n",
    "        #   hidden [B x hidden_size Tensor], which are activations in the Hidden Layer\n",
    "        #   rep [B x rep_size Tensor], which are the activations in the Representation LAyer\n",
    "        x = x.view(-1,nobj+nrel) # reshape as size [B x (nobj+nrel) Tensor] if B=1\n",
    "        x_item = x[:,:nobj] # input to Item Layer [B x nobj Tensor]\n",
    "        x_rel = x[:,nobj:] # input to Relation Layer [B x nrel Tensor]\n",
    "        # TODO : YOUR CODE GOES HERE\n",
    "        # ----\n",
    "        raise Exception('Replace with your code.')\n",
    "        # -----\n",
    "        return output, hidden, rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a completed function `train` for stochastic gradient descent. The network makes online (rather than batch) updates, adjusting its weights after the presentation of each input pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mynet,epoch_count,nepochs_additional=5000):\n",
    "    # Input\n",
    "    #  mynet : Net class object\n",
    "    #  epoch_count : (scalar) how many epochs have been completed so far\n",
    "    #  nepochs_additional : (scalar) how many more epochs we want to run\n",
    "    mynet.train()\n",
    "    for e in range(nepochs_additional): # for each epoch\n",
    "        error_epoch = 0.\n",
    "        perm = np.random.permutation(N)\n",
    "        for p in perm: # iterate through input patterns in random order\n",
    "            mynet.zero_grad() # reset gradient\n",
    "            output, hidden, rep = mynet(input_pats[p,:]) # forward pass\n",
    "            target = output_pats[p,:] \n",
    "            loss = criterion(output, target) # compute loss\n",
    "            loss.backward() # compute gradient \n",
    "            optimizer.step() # update network parameters\n",
    "            error_epoch += loss.item()\n",
    "        error_epoch = error_epoch / float(N)        \n",
    "        if e % 50 == 0:\n",
    "            print('epoch ' + str(epoch_count+e) + ' loss ' + str(round(error_epoch,3)))\n",
    "    return epoch_count + nepochs_additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide some useful functions for extracting the activation pattern on the Representation Layer for each possible item. We provide two functions `plot_rep` and `plot_dendo` for visualizing these activation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep(net):\n",
    "    # Extract the hidden activations on the Representation Layer for each item\n",
    "    # \n",
    "    # Input\n",
    "    #  net : Net class object\n",
    "    #\n",
    "    # Output\n",
    "    #  rep : [nitem x rep_size numpy array], where each row is an item\n",
    "    input_clean = torch.zeros(nobj,nobj+nrel)\n",
    "    for idx,name in enumerate(names_items):\n",
    "        input_clean[idx,idx] = 1. # 1-hot encoding of each object (while Relation Layer doesn't matter)\n",
    "    output, hidden, rep = mynet(input_clean)\n",
    "    return rep.detach().numpy()\n",
    "\n",
    "def plot_rep(rep1,rep2,rep3,names):\n",
    "    #  Compares Representation Layer activations of Items at three different times points in learning (rep1, rep2, rep3)\n",
    "    #  using bar graphs\n",
    "    # \n",
    "    #  Each rep1, rep2, rep3 is a [nitem x rep_size numpy array]\n",
    "    #  names : [nitem list] of item names\n",
    "    #\n",
    "    nepochs_list = [nepochs_phase1,nepochs_phase2,nepochs_phase3]\n",
    "    nrows = nobj\n",
    "    R = np.dstack((rep1,rep2,rep3))    \n",
    "    mx = R.max()\n",
    "    mn = R.min()\n",
    "    depth = R.shape[2]\n",
    "    count = 1\n",
    "    plt.figure(1,figsize=(4.2,8.4))\n",
    "    for i in range(nrows):\n",
    "        for d in range(R.shape[2]):\n",
    "            plt.subplot(nrows, depth, count)\n",
    "            rep = R[i,:,d]\n",
    "            plt.bar(range(rep.size),rep)\n",
    "            plt.ylim([mn,mx])\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])        \n",
    "            if d==0:\n",
    "                plt.ylabel(names[i])\n",
    "            if i==0:\n",
    "                plt.title(\"epoch \" + str(nepochs_list[d]))\n",
    "            count += 1\n",
    "    plt.show()\n",
    "\n",
    "def plot_dendo(rep1,rep2,rep3,names):\n",
    "    #  Compares Representation Layer activations of Items at three different times points in learning (rep1, rep2, rep3)\n",
    "    #  using hierarchical clustering\n",
    "    # \n",
    "    #  Each rep1, rep2, rep3 is a [nitem x rep_size numpy array]\n",
    "    #  names : [nitem list] of item names\n",
    "    #\n",
    "    nepochs_list = [nepochs_phase1,nepochs_phase2,nepochs_phase3]\n",
    "    linked1 = linkage(rep1,'single')\n",
    "    linked2 = linkage(rep2,'single')\n",
    "    linked3 = linkage(rep3,'single')\n",
    "    mx = np.dstack((linked1[:,2],linked2[:,2],linked3[:,2])).max()+0.1    \n",
    "    plt.figure(2,figsize=(7,12))\n",
    "    plt.subplot(3,1,1)    \n",
    "    dendrogram(linked1, labels=names, color_threshold=0)\n",
    "    plt.ylim([0,mx])\n",
    "    plt.title('Hierarchical clustering; ' + \"epoch \" + str(nepochs_list[0]))\n",
    "    plt.ylabel('Euclidean distance')\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.title(\"epoch \" + str(nepochs_list[1]))\n",
    "    dendrogram(linked2, labels=names, color_threshold=0)\n",
    "    plt.ylim([0,mx])\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.title(\"epoch \" + str(nepochs_list[2]))\n",
    "    dendrogram(linked3, labels=names, color_threshold=0)\n",
    "    plt.ylim([0,mx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next script initializes the neural network and trains it for 4000 epochs total. It trains in three stages, and the item representations (on the Representation Layer) are extracted after 500 epochs, 1500 epochs, and then at the end of training (4000 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "criterion = nn.MSELoss() # mean squared error loss function\n",
    "mynet = Net(rep_size=8,hidden_size=15)\n",
    "optimizer = torch.optim.SGD(mynet.parameters(), lr=learning_rate) # stochastic gradient descent\n",
    "\n",
    "nepochs_phase1 = 500\n",
    "nepochs_phase2 = 1000\n",
    "nepochs_phase3 = 2500\n",
    "epoch_count = 0\n",
    "epoch_count = train(mynet,epoch_count,nepochs_additional=nepochs_phase1)\n",
    "rep1 = get_rep(mynet)\n",
    "epoch_count = train(mynet,epoch_count,nepochs_additional=nepochs_phase2-nepochs_phase1)\n",
    "rep2 = get_rep(mynet)\n",
    "epoch_count = train(mynet,epoch_count,nepochs_additional=nepochs_phase3-nepochs_phase2)\n",
    "rep3 = get_rep(mynet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the Representation Layer at the different stages of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rep(rep1,rep2,rep3,names_items)\n",
    "plot_dendo(rep1,rep2,rep3,names_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 2 (5 points) </h3>\n",
    "<br>\n",
    "Based on your plots, write a short analysis (4-5 sentences) of how the internal representations of the network develop over the course of learning. How does learning progress? Does the network start by differentiating certain classes of patterns from each other, and then differentiate others in later stages?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Hint: You can refer to your lecture slides and notes for the R&M model for help with your analysis. Your network should broadly replicate their findings, but since the training patterns and activation function aren't identical, don't expect the exact same results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in homework\n",
    "\n",
    "When you are finished with this notebook. Save your work in order to turn it in.  To do this select *File*->*Download As...*->*PDF*.\n",
    "\n",
    "<img src=\"images/save-pdf.png\" width=\"300\">\n",
    "\n",
    "You can turn in your assignments using NYU Classes webpage for the course (available on https://home.nyu.edu). **Make sure you complete all parts (A-E) of this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
