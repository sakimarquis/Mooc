{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Neural networks - Part D (20 points)\n",
    "## Predicting similarity ratings with a deep convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by *Brenden Lake* and *Todd Gureckis*  \n",
    "Computational Cognitive Modeling  \n",
    "NYU class webpage: https://brendenlake.github.io/CCM-site/  \n",
    "email to course instructors: instructors-ccm-spring2020@nyuccl.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "This homework is due before midnight on Monday Feb. 24, 2020.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Summary:** We will examine whether deep convolutional neural networks (convnets) trained for image classification can predict human similarity ratings for handwritten digits. The images below are all clear examples of the digit '8'. Nonetheless, the left two '8's look more \"similar\" to me than the right two '8's. Can a neural network trained for classification help to explain judgments such as this one?\n",
    "\n",
    "In this notebook, we will use a pre-trained convnet for digit recognition to predict similarity judgments. We will then test the network's predictions by collecting similarity ratings from a couple of friends. *This assignment requires collecting a small amount of behavioral data, so grab a friend and don't wait to the last minute to get started!*\n",
    "\n",
    "|High similarity&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|Low similarity&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n",
    "|:------|:------|\n",
    "|<img src=\"images/pair2.png\" width=150px>|<img src=\"images/pair1.png\" width=150px>|\n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "The goal of this assignment is to give you some hands-on experience with a powerful toolkit for computer vision (deep convnets), which has also recently been applied for studying human perception and categorization.\n",
    "\n",
    "In 2012, Krizhevsky, Sutskever, and Hinton trained a deep convnet (now called 'AlexNet') for object recognition and achieved very impressive results, reducing the number of errors on ImageNet by almost half compared to the next best approach. This paper ignited the recent deep learning revolution in computer vision, although convnets were being used for this purpose long before (LeCun et al. (1989) was the first to train deep convnets for digit recognition).\n",
    "\n",
    "Recently, the success of convnets has led researchers in cognitive science and neuroscience to explore their potential applications for understanding human visual perception and categorization. Here are some examples: Yamins et al. (2014) showed that deep convnets can predict neural response in high-level visual areas; Lake, Zaremba, Fergus, and Gureckis (2015) showed that deep convnets can explain human typicality ratings for some classes of natural images; Peterson, Abbott, and Griffiths (2016) explored convents for predicting similarity ratings between images.\n",
    "\n",
    "In this assignment, like Peterson et al., we will explore convnets for predicting human similarity judgments between images. We use a relatively small-scale network trained for digit recognition, but the same principles can be used to study much more complex deep convnets trained for object recognition. Although \"similarity\" can be a tricky construct in cognitive science (Medin, Goldstone, & Gentner, 1993), similarity judgments are still a useful tool in a cognitive scientist's toolbox for understanding representation.\n",
    "\n",
    "**References:**\n",
    "* Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems.\n",
    "* Lake, B. M., Zaremba, W., Fergus, R., & Gureckis, T. M. (2015). Deep Neural Networks Predict Category Typicality Ratings for Images. In Proceedings of the Cognitive Science Society.\n",
    "* LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4), 541-551.\n",
    "* Medin, D. L., Goldstone, R. L., & Gentner, D. (1993). Respects for similarity. Psychological Review, 100(2), 254.\n",
    "* Peterson, J. C., Abbott, J. T., & Griffiths, T. L. (2016). Adapting deep network features to capture psychological representations. arXiv preprint arXiv:1608.02164.\n",
    "* Yamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A., Seibert, D., & DiCarlo, J. J. (2014). Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the National Academy of Sciences, 111(23), 8619-8624."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading MNIST\n",
    "\n",
    "MNIST is one of the most famous data sets in machine learning. It is a digit recognition task, where the goal is classify images of handwritten digits with right label, e.g, either '0','1','2', ... '9'. The training set consists of 60,000 images (6,000 per digit), and the test set of has 10,000 additional images.\n",
    "\n",
    "Execute the code below to load some libraries and the MNIST \"test set.\" We will not need the training set, since we will use a pre-trained network for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "\n",
    "print('Loading MNIST')\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('data',\n",
    "                                                            train=False,\n",
    "                                                            download=True,\n",
    "                                                            transform=transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "                                                        ),batch_size=1000, shuffle=True)\n",
    "print('MNIST has been loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convnet architecture for digit recognition\n",
    "Here is the `Net` class defining the network we are using for digit recognition.\n",
    "\n",
    "**First convolutional layer:**  \n",
    "This layer takes an image (28x28 for MNIST) and applies a bank of learnable 5x5 filters to produce 10 new images (also known as channels or feature maps). Each feature map is passed through a non-linearity (a rectified linear unit or \"ReLU\"). Last, there is a max pooling operation that reduces the size of each feature map by half.\n",
    "\n",
    "**Second convolutional layer:**  \n",
    "This layer takes the feature maps from the previous layer, applies a bank of learnable 5x5 filters, and produces 20 new feature maps. Again, ReLU's are applied as well as max pooling.\n",
    "\n",
    "**Fully-connected layer:**  \n",
    "Next is a standard fully-connected layer of 50 units. At this stage, the entire image is summarized with a vector of size 50. ReLu's are used again.\n",
    "\n",
    "**Output layer:**\n",
    "The output layer has 10 units with one to represent each of the 10 digits. It uses a softmax activation function, to ensure the network's predictions are a valid probability distribution of the 10 possibilities.\n",
    "\n",
    "Execute the code to define the `Net` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # 10 channels in first convolution layer\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # 20 channels in second conv. layer\n",
    "        self.fc1 = nn.Linear(320, 50) # 50 hidden units in first fully-connected layer\n",
    "        self.fc2 = nn.Linear(50, 10) # 10 output units\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # first convolutional layer\n",
    "        h_conv1 = self.conv1(x)\n",
    "        h_conv1 = F.relu(h_conv1)\n",
    "        h_conv1_pool = F.max_pool2d(h_conv1, 2)\n",
    "\n",
    "        # second convolutional layer\n",
    "        h_conv2 = self.conv2(h_conv1_pool)\n",
    "        h_conv2 = F.relu(h_conv2)\n",
    "        h_conv2_pool = F.max_pool2d(h_conv2, 2)\n",
    "\n",
    "        # fully-connected layer\n",
    "        h_fc1 = h_conv2_pool.view(-1, 320)\n",
    "        h_fc1 = self.fc1(h_fc1)\n",
    "        h_fc1 = F.relu(h_fc1)\n",
    "        \n",
    "        # classifier output\n",
    "        output = self.fc2(h_fc1)\n",
    "        output = F.log_softmax(output,dim=1)\n",
    "        return output, h_fc1, h_conv2, h_conv1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating classification performance of the network\n",
    "\n",
    "Here we will execute our first bit of non-trivial code. To save time, we already trained a the MNIST network for you. The code below loads the pre-trained model. If you are curious, it only takes a couple minutes to train a MNIST model from scratch on a standard laptop, and we used a version of the PyTorch code from [here](https://github.com/pytorch/examples/tree/master/mnist) to train this network.\n",
    "\n",
    "After loading the network, the function `test_all` iterates through the whole test set and computes the network's predicted class for each image. It outputs the percent correct.\n",
    "\n",
    "The function `test_viz` picks a few images from the test set at random and displays them along with the network's predicted class labels.\n",
    "\n",
    "Execute the code below. If everything is working right, **you should get test performance of 98.71%** correct which isn't state-of-the-art, but is reasonable. You can also see some of the network's specific predictions on some test images, which should also look good. You can re-run to see a few different sets of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classification accuracy on the entire MNIST test set\n",
    "def test_all():\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # run the data through the network\n",
    "        output, h_fc1, h_conv2, h_conv1 = model(data)\n",
    "        # compare prediction to ground truth\n",
    "        pred = torch.max(output,dim=1)[1] # get the index of the max log-probability\n",
    "        correct += torch.eq(pred,target.view_as(pred)).cpu().sum().item()\n",
    "    perc_correct = 100. * correct / len(test_loader.dataset)\n",
    "    return perc_correct\n",
    "\n",
    "# Show the network's predicted class for an arbitrary set of `nshow` images from the MNIST test set\n",
    "def test_viz(nshow=10):\n",
    "    \n",
    "    # grab a random subset of the data\n",
    "    testiter = iter(test_loader)\n",
    "    images, target = testiter.next()\n",
    "    perm = np.random.permutation(images.shape[0])\n",
    "    data = images[perm[:nshow]]\n",
    "    \n",
    "    # get predictions from the network\n",
    "    output, h_fc1, h_conv2, h_conv1 = model(data)\n",
    "    pred = torch.max(output,dim=1)[1]\n",
    "    pred = pred.numpy().flatten()\n",
    "    \n",
    "    # plot predictions along with the images\n",
    "    for i in range(nshow):\n",
    "        ax = plt.subplot(1, nshow, i+1)\n",
    "        imshow(utils.make_grid(data[i]))\n",
    "        plt.title(str(pred[i]))\n",
    "\n",
    "# Display an image from the MNIST data set\n",
    "def imshow(img):\n",
    "    img = 1 - (img * 0.3081 + 0.1307) # invert image pre-processing\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "checkpoint = torch.load('models/convnet_mnist_pretrained.pt')\n",
    "model = Net()\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "print('Convnet has been loaded successfully.')\n",
    "print('Running through the test set...')\n",
    "test_acc = test_all()\n",
    "print(' Accuracy on the test set is %.2f percent correct!' % test_acc)\n",
    "print(\"\")\n",
    "print(\"Here are some predictions from the network.\")\n",
    "print(\"The images are shown below their predicted class labels.\")\n",
    "test_viz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting digits for similarity analysis\n",
    "\n",
    "Here are two more useful functions: `get_random_subset` and `plot_image_pairs.`\n",
    "\n",
    "As mentioned, we are looking at within-class similarity comparisons (e.g., comparing two different images of the digit '8'). The function `get_random_subset` generates a set of random images pairs from a particular image class `digit_select` from the MNIST test set. The number of random pairs can be set by `npairs`. \n",
    "\n",
    "The other function is `plot_image_pairs` which visualizes each of the random image pairs. The pairs are input as two list of image tensors, where images with the same index are paired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_subset(digit_select, npairs=20):\n",
    "    # digit_select: which digit do we want to get images for?\n",
    "    testiter = iter(test_loader)\n",
    "    images, target = testiter.next()\n",
    "    indices = np.flatnonzero(target.numpy() == digit_select)    \n",
    "    np.random.shuffle(indices)\n",
    "    indx1 = torch.tensor(indices[:npairs],dtype=torch.long)\n",
    "    indx2 = torch.tensor(indices[npairs:npairs*2],dtype=torch.long)\n",
    "    images1 = images[indx1]\n",
    "    images2 = images[indx2]\n",
    "    plt.figure(1,figsize=(4,40))\n",
    "    plot_image_pairs(images1,images2)\n",
    "    return images1, images2\n",
    "\n",
    "def plot_image_pairs(images1,images2,scores_net=[],scores_people=[]):\n",
    "    # images1 : list of images (each image is a tensor)\n",
    "    # images2 : list of images (each image is a tensor)\n",
    "    # scores_net (optional) : network similarity score for each pair\n",
    "    # scores_people (optional) : human similarity score for each pair\n",
    "    npairs = images1.size()[0]\n",
    "    assert images2.size()[0] == npairs\n",
    "    for i in range(npairs):\n",
    "        ax = plt.subplot(npairs, 1, i+1)\n",
    "        imshow(utils.make_grid([images1[i], images2[i]]))\n",
    "        mytitle = ''\n",
    "        if len(scores_net)>0:\n",
    "            mytitle += 'net %.2f, ' % scores_net[i] \n",
    "        if len(scores_people)>0:\n",
    "            mytitle += 'human. %.2f' % scores_people[i]\n",
    "        if mytitle:\n",
    "            plt.title(mytitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is code for sampling 20 random pairings of images of the digit '8' (or whichever digit you set `digit_select` to). Run to sample the pairings and then visualize them (you may have to scroll). \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "To save or print out the image pairs, which will be helpful later in the assignment, you can right click and save the output as an image.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images1_digit8,images2_digit8 = get_random_subset(digit_select=8, npairs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing similarity judgments with the network\n",
    "To get predictions out of the network, we take the hidden representation at a particular layer as a representation of an image. The hidden representation computes high-level features of the images, which we will use to compare the images. To compute the similarity between two images, we get the two hidden vectors and compare them with the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) metric. Cosine similarity measures the angle between two vectors, which can be an effective way of measuring similarity between patterns of activation on the hidden layer.\n",
    "\n",
    "The function `get_sim_judgments` takes two lists of images and computes the similarity between each pair. Using the `layer` parameter, you can choose between the fully connected layer (`fc`), first convolutional layer (`conv1`), or second convolutional layer (`conv2`).\n",
    "\n",
    "The function `normalize` rescales a vector to have a minimum value of 0 and maximum value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-scale a vector to have a minimum value of 0, maximum of 1\n",
    "def normalize(v):\n",
    "    # v : numpy vector\n",
    "    v = v - v.min()\n",
    "    v = v / v.max()\n",
    "    return v\n",
    "\n",
    "# Compute convnet similarity for each pair of images\n",
    "def get_sim_judgments(images1,images2,layer='fc'):\n",
    "    # images1 : list of N images (each image is a tensor)\n",
    "    # images2 : list of N images (each image is a tensor)\n",
    "    # layer : which layer do we want to use? fully connected ('fc'), \n",
    "    #         first convolutional ('conv1'), or second convolutional ('conv2')\n",
    "    #\n",
    "    # Return\n",
    "    #  v_sim : N-dimensional vector\n",
    "    N = images1.size()[0] # number of pairs\n",
    "    assert N == images2.size()[0]\n",
    "    output_1, h_fc1_1, h_conv2_1, h_conv1_1 = model(images1)\n",
    "    output_2, h_fc1_2, h_conv2_2, h_conv1_2 = model(images2)\n",
    "    \n",
    "    # grab the tensors from the appropriate layer\n",
    "    if layer=='fc':\n",
    "        T1 = h_fc1_1\n",
    "        T2 = h_fc1_2\n",
    "    elif layer=='conv1':\n",
    "        T1 = h_conv1_1\n",
    "        T2 = h_conv1_2\n",
    "    elif layer=='conv2':\n",
    "        T1 = h_conv2_1\n",
    "        T2 = h_conv2_2\n",
    "    else:\n",
    "        raise Exception('Layer parameter has unrecognized value')\n",
    "    \n",
    "    # flatten the tensors for each image\n",
    "    T1 = T1.detach().numpy().reshape(N,-1)\n",
    "    T2 = T2.detach().numpy().reshape(N,-1)\n",
    "\n",
    "    v_sim = np.zeros(N)\n",
    "    for i in range(N): # for each pair\n",
    "        v1 = T1[i,:]\n",
    "        v2 = T2[i,:]\n",
    "        v_sim[i] = 1-cosine(v1,v2) # using cosine distance \n",
    "    return v_sim\n",
    "\n",
    "# Get similarity bsaed on the fully conneted layer at the end\n",
    "v_sim_net_digit8 = get_sim_judgments(images1_digit8,images2_digit8,'fc')\n",
    "print(\"Similarity ratings from the neural network the for digit 8:\")\n",
    "print(v_sim_net_digit8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> Problem 1 (10 points) </h2>\n",
    "<br>\n",
    "Go through the following steps for the digit '8':\n",
    "<ol>\n",
    "<li> *Get judgments from the network.* Run all of the code above to sample a list of 20 pairs of images for that digit. For each pair, get a measure of the network's similarity between the images as measured on the fully-connected `fc` layer. (You may have already done all of this.)</li><br>\n",
    "<li>*Get judgments from people.* Ask two people to rate the same image pairs on a 1 (least similar) to 10 (most similar) scale, using the question \"How similar do these two images look to you?\" An easy way to collect the data is to print out the image pairs (you can right-click and save the jupyter notebook figures as images). Ask your participant to write their ratings next to each pair. Enter the ratings in the arrays `ratings_human_1_digit8` and `ratings_human_2_digit8` below. One participant can be yourself, but the other needs to be someone else.</li><br>\n",
    "<li>*Compare.* Run the code below to compute the correlation coefficient and scatter plot.</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PARTICIPANT RATINGS GO HERE\n",
    "ratings_human_1_digit8 = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], dtype='float')\n",
    "ratings_human_2_digit8 = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0], dtype='float')\n",
    "\n",
    "# Analysis code\n",
    "ratings_human_mean_digit8 = (ratings_human_1_digit8 + ratings_human_2_digit8)/2\n",
    "\n",
    "v_sim_net_norm = normalize(v_sim_net_digit8)\n",
    "v_sim_human_norm = normalize(ratings_human_mean_digit8)\n",
    "\n",
    "print(\"Correlation between net and human similarity ratings: r =\"),\n",
    "print(round(np.corrcoef(v_sim_net_norm,v_sim_human_norm)[0][1],3))\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(1,figsize=(5,5))\n",
    "plt.plot(v_sim_net_norm,v_sim_human_norm,'k.')\n",
    "plt.xlabel('network similarity (normalized)')\n",
    "plt.ylabel('human similarity (normalized)')\n",
    "plt.title('Convnet vs. human similarity judgments (digit 8)')\n",
    "plt.show()\n",
    "\n",
    "# pairs with similarity ratings\n",
    "plt.figure(2,figsize=(4,40))\n",
    "plot_image_pairs(images1_digit8,images2_digit8,v_sim_net_norm,v_sim_human_norm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h2> Problem 2 (10 points) </h2>\n",
    "<br>\n",
    "How well does the convolutional neural network do at predicting the similarity ratings? What is it capturing? What is it not capturing? (Keep your answer brief. About 5 sentences is good.)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Don't worry if your fit isn't very good! A convnet is very far from a perfect account of human similarity ratings. I had reasonable success with the `8` digit but I wouldn't except a correlation higher than r=0.6. This assignment is not graded on how good the fit is.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in homeworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finished with this notebook. Save your work in order to turn it in.  To do this select *File*->*Download As...*->*PDF*.\n",
    "\n",
    "<img src=\"images/save-pdf.png\" width=\"300\">\n",
    "\n",
    "You can turn in your assignments using NYU Classes webpage for the course (available on https://home.nyu.edu). **Make sure you complete all parts (A-E) of this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
