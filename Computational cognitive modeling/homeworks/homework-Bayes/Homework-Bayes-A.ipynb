{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Bayesian modeling - Part A (80 points)\n",
    "## Bayesian concept learning with the number game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by *Brenden Lake* and *Todd Gureckis*  \n",
    "Computational Cognitive Modeling  \n",
    "NYU class webpage: https://brendenlake.github.io/CCM-site/  \n",
    "email to course instructors: instructors-ccm-spring2020@nyuccl.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "This homework is due before midnight on Monday, April 13. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will get hands on experience with Bayesian concept learning and the \"number game,\" as covered in lecture. As with so many of our everyday inferences, the data we receive is far too sparse and noisy to be conclusive. Nevertheless, people must make generalizations and take actions based on imperfect and insufficient data. In data science and machine learning, the situation is often the same: the data is not enough to produce an answer with certainty, yet we can make meaningful generalizations anyway. What computational mechanisms can support these types of inferences?\n",
    "\n",
    "The number game is a quintessential inductive problem. In the number game, there is an unknown computer program that generates numbers in the range 1 to 100. You are provided with a small set of random examples from this program. For instance, in the figure below, you get two random examples from the program: the numbers '8' and '2'.\n",
    "\n",
    "<img src=\"images/number_game_comp.jpeg\" style=\"width: 300px;\"/>\n",
    "\n",
    "Which numbers will also be accepted by the same program? Of course, it depends what the program is, and you don't have enough information to be sure. Should '9' be accepted? Perhaps, if the concept is \"all numbers up to 10.\" What about '10'? A better candidate, since the program could again be \"numbers up to 10\", or \"all even numbers.\" What about '16'? This is another good candidate, and the program \"powers of 2\" is also consistent with the examples so far. How should one generalize based on the evidence so far? This homework explores how the Bayesian framework provides an answer to this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "The number game was introduced in the following paper:\n",
    "<ul>\n",
    "<li>Tenenbaum, J. B. (2000). Rules and similarity in concept learning. In Advances in Neural Information Processing Systems (NIPS).</li>\n",
    "</ul>\n",
    "This is assignment is adapted from exercises developed by Josh Tenenbaum.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayesian model\n",
    "In the number game, we receive a set of $n$ positive examples $X = \\{x^{(1)},...,x^{(n)}\\}$ of an unknown concept $C$. In a Bayesian analysis of the task, the goal is predict $P(y \\in C\\ |\\ X)$, which is the probability that a new number $y$ is also a member of the concept $C$ after receiving the set of examples $X$.\n",
    "\n",
    "#### Updating beliefs with Bayes' rule\n",
    "Let's proceed with the Bayesian model of the task. There is a hypothesis space $H$ of concepts, where a particular member of the hypothesis space (i.e., a particular concept) is denoted $h \\in H$. The Bayesian model includes a prior distribution $P(h)$ over the hypotheses and a likelihood $P(X|h)$. Bayes' rule specifies how to compute the posterior distribution over hypotheses given these two pieces:\n",
    "\\begin{equation}\n",
    "P(h|X) = \\frac{P(X|h)P(h)}{\\sum_{h' \\in H} P(X|h')P(h')}\n",
    "\\end{equation}\n",
    "The likelihood is specified below. We will leave the prior to later.\n",
    "\n",
    "#### Likelihood\n",
    "We assume that each number in $X$ is an independent sample from the set of all valid numbers. Thus, the likelihood decomposes as a product of individual probabilities,\n",
    "\\begin{equation}\n",
    "P(X|h) = \\prod_{i=1}^n P(x^{(i)}|h).\n",
    "\\end{equation}\n",
    "We assume that the numbers are sampled uniformly at random from the set of valid numbers, such that $P(x^{(i)}|h) = \\frac{1}{|h|}$ if $x^{(i)} \\in h$ and $P(x^{(i)}|h) = 0$ otherwise. The term $|h|$ is the cardinality or set size of the hypothesis $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 1 (10 points) </h3>\n",
    "<br>\n",
    "Let's compute a very simple posterior distribution by hand. Assume there are only two possible hypotheses:\n",
    "<ul>\n",
    "    <li>multiples of 10 ($h_1$)</li>\n",
    "    <li>even numbers ($h_2$)</li>\n",
    "</ul>\n",
    "<br>\n",
    "which are equally likely in the prior. Only the numbers 1 through 100 are possible. The positive examples $X$ consist of just 10 and 30. What is the posterior probability of each hypothesis? Please **show your work** in the cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER GOES HERE\n",
    "\n",
    "You can create multi-line equations as follows:\n",
    "\\begin{equation}\n",
    "1+1 = 2 \\\\\n",
    "2+2 = 4 \\\\\n",
    "y = x^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making posterior predictions\n",
    "Once we have the posterior beliefs over hypotheses, we want to be able to make predictions about the membership of a new number $y$ in the concept $C$, or as mentioned $P(y \\in C\\ |\\ X)$. To compute this, we average over all possible hypotheses weighted by the posterior probability,\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\in C\\ |\\ X) = \\sum_{h \\in H} P(y \\in C\\ |\\ h) P(h|X),\n",
    "\\end{equation}\n",
    "\n",
    "where the first term is simply $1$ or $0$ based on the membership of $y$ in h, and the second term is the posterior weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 2 (10 points) </h3>\n",
    "<br>\n",
    "Now let's manually compute some predictions.\n",
    "<ul>\n",
    "<li> Given the posterior distribution computed in Problem 1, what is the probability that `40` is also a member of the concept? </li> \n",
    "<li> Given the same posterior, what is the probability that `4` is also a member of the concept? </li> \n",
    "</ul>\n",
    "<br>\n",
    "Please **show your work** in the cell below. Your answers from Problem 1 will help you.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation - Hypothesis space and prior\n",
    "Let's dive into the implementation of the number game. First, let's import some packages and helpful functions.\n",
    "\n",
    "`x_all` is the list of all possible numbers. We include 0 as a possible number for programming convenience, although none of the hypotheses include it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "x_max = 100 # maximum number allowed\n",
    "x_all = np.arange(0,x_max+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis space $H$ includes two main kinds of hypotheses. The first kind consists of mathematical hypotheses such as odd numbers, even numbers, square numbers, cube numbers, primes, multiples of $n$, powers of $n$, and numbers ending with a particular digit. The second kind consists of interval hypotheses, which are solid intervals of numbers, such as $12, 13, 14, 15, 16, 17$. Each hypothesis will be represented as a list of the numbers that fit that hypothesis. The free parameters `mylambda` controls how much of the prior is specified by each type of hypothesis, with `mylambda` weight going to the mathematical hypotheses and `1-mylambda` weights going to the interval hypotheses.\n",
    "\n",
    "The code below shows how to generate the mathematical hypotheses and their prior probabilities (in natural log space).  To keep the prior simple, each mathematical hypothesis is given equal weight in the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_h_odd():\n",
    "    return list(range(1,x_max+1,2))\n",
    "\n",
    "def make_h_even():\n",
    "    return list(range(2,x_max+1,2))\n",
    "\n",
    "def make_h_square():\n",
    "    h = []\n",
    "    for x in range(1,x_max+1):\n",
    "        if x**2 <= x_max:\n",
    "            h.append(x**2)\n",
    "    return h\n",
    "\n",
    "def make_h_cube():\n",
    "    h = []\n",
    "    for x in range(1,x_max+1):\n",
    "        if x**3 <= x_max:\n",
    "            h.append(x**3)\n",
    "    return h\n",
    "\n",
    "def make_h_primes():\n",
    "    return [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
    "\n",
    "def make_h_mult_of_y(y):\n",
    "    h = []\n",
    "    for x in range(1,x_max+1):\n",
    "        if x*y <= x_max:\n",
    "            h.append(x*y)\n",
    "    return h\n",
    "\n",
    "def make_h_powers_of_y(y):\n",
    "    h = []\n",
    "    for x in range(1,x_max+1):\n",
    "        if y**x <= x_max:\n",
    "            h.append(y**x)\n",
    "    return h\n",
    "\n",
    "def make_h_numbers_ending_in_y(y):\n",
    "    h = []\n",
    "    for x in range(1,x_max+1):\n",
    "        if str(x)[-1] == str(y):\n",
    "            h.append(x)\n",
    "    return h\n",
    "\n",
    "def generate_math_hypotheses(mylambda):\n",
    "    h_set = [make_h_odd(), make_h_even(), make_h_square(), make_h_cube(), make_h_primes()]\n",
    "    h_set += [make_h_mult_of_y(y) for y in range(3,13)]\n",
    "    h_set += [make_h_powers_of_y(y) for y in range(2,11)]\n",
    "    h_set += [make_h_numbers_ending_in_y(y) for y in range(0,10)]\n",
    "    n_hyp = len(h_set)\n",
    "    log_prior = np.log(mylambda * np.ones(n_hyp) / float(n_hyp))\n",
    "    return h_set, log_prior\n",
    "\n",
    "h_set_math, log_prior_math = generate_math_hypotheses(2./3)\n",
    "print(\"Four examples of math hypotheses:\")\n",
    "for i in range(4):\n",
    "    print(h_set_math[i])\n",
    "    print(\"\")\n",
    "print(\"Their prior log-probabilities:\")\n",
    "print(log_prior_math[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All possible interval hypotheses and their prior probabilities can be generated with the following code. All interval hypotheses are not equally likely in the prior, and following Tenenbaum's specification, we use an Erlang distribution with parameter `sigma=10` to express an expectation for intermediate-sized hypotheses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_h_between_y_and_z(y,z):\n",
    "    assert(y >= 1 and z <= x_max)\n",
    "    return list(range(y,z+1))\n",
    "\n",
    "def pdf_erlang(x,sigma=10.):\n",
    "    return (x / sigma**2) * np.exp(-x/sigma)\n",
    "\n",
    "def generate_interval_hypotheses(mylambda):\n",
    "    h_set = []\n",
    "    for y in range(1,x_max+1):\n",
    "        for z in range(y,x_max+1):            \n",
    "            h_set.append(make_h_between_y_and_z(y,z))\n",
    "    nh = len(h_set)\n",
    "    pv = np.ones(nh)\n",
    "    for idx,h in enumerate(h_set): # prior based on length\n",
    "        pv[idx] = pdf_erlang(len(h))\n",
    "    pv = pv / np.sum(pv)\n",
    "    pv = (1-mylambda) * pv\n",
    "    log_prior = np.log(pv)\n",
    "    return h_set, log_prior\n",
    "\n",
    "h_set_int, log_prior_int = generate_interval_hypotheses(2./3)\n",
    "print(\"Four examples of interval hypotheses\")\n",
    "for i in range(4):\n",
    "    print(h_set_int[i])\n",
    "    print(\"\")\n",
    "print(\"Their prior log-probabilities:\")\n",
    "print(log_prior_int[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, we can define a `generate_hypotheses` function that uses all of this code to generate the complete set of hypotheses and their prior probabilities. We also use `convert_h_list_to_numpy` to convert each hypothesis from a Python list of numbers to a binary numpy array for speedier Bayesian computations later (run code to see example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_h_list_to_numpy(h_list):\n",
    "    h_numpy = np.zeros(x_all.size)\n",
    "    h_numpy[np.array(h_list)] = 1\n",
    "    return h_numpy\n",
    "\n",
    "def generate_hypotheses(mylambda):\n",
    "    h_math,lp_math = generate_math_hypotheses(mylambda)\n",
    "    h_interval,lp_interval = generate_interval_hypotheses(mylambda)\n",
    "    H = h_math + h_interval\n",
    "    H_numpy = [convert_h_list_to_numpy(h) for h in H]\n",
    "    log_prior = np.concatenate((lp_math,lp_interval))\n",
    "    assert(np.isclose(np.sum(np.exp(log_prior)),1.0))\n",
    "    return H_numpy,log_prior\n",
    "\n",
    "print(\"Example of converting list hypothesis to numpy array...\")\n",
    "print(\"original hypothesis:\")\n",
    "h_list = [2,4,6]\n",
    "print(h_list)\n",
    "h_numpy = convert_h_list_to_numpy(h_list)\n",
    "print(\"converted numpy array:\")\n",
    "print(h_numpy[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation - Posterior and likelihood\n",
    "Now we need code to do the Bayesian computations, including a function `log_posterior` and `log-likelihood`. For probabilistic modeling, we like to compute probabilities in log-space to help avoid numerical issues such as underflow. Study the function `log_posterior` to make sure you understand how it works. Also, see the nifty `logsumexp` function ([see scipy doc](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.misc.logsumexp.html)) which is used to normalize log-probability distributions in a numerically safer way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(data,list_hypothesis,log_prior):\n",
    "    # INPUT\n",
    "    #  data : python list of observed numbers (X) \n",
    "    #  list_hypothesis : [nh length list] each hypothesis is a binary numpy array \n",
    "    #  log_prior : numpy vector [length nh] (log prior value for each hypothesis)\n",
    "    data_numpy = convert_h_list_to_numpy(data) # length nh numpy vector\n",
    "    nh = len(list_hypothesis)\n",
    "    ll = np.zeros(nh)\n",
    "    for idx,h in enumerate(list_hypothesis):\n",
    "        ll[idx] = log_likelihood(data_numpy,h)\n",
    "    lpost = ll + log_prior\n",
    "    lpost = lpost - logsumexp(lpost)\n",
    "    return lpost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 3 (10 points) </h3>\n",
    "<br>\n",
    "Fill in the missing code below to complete the `log-likelihood` function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(data_numpy, hypothesis):\n",
    "    # INPUT\n",
    "    #  data_numpy : size x_max binary numpy array (observed numbers)\n",
    "    #  hypothesis: size x_max binary numpy array (included numbers in single hypothesis)\n",
    "    # RETURN\n",
    "    #  ll : log-likelihood value (REMEMBER TO CONVERT TO NATURAL LOG (np.log()))\n",
    "    assert(hypothesis.size == data_numpy.size)\n",
    "    n_d = np.sum(data_numpy)\n",
    "    n_h = np.sum(hypothesis)\n",
    "\n",
    "    # TODO: Add your code to check whether or not the hypothesis contains all of the data.\n",
    "    #   If it does not, return -np.inf (log(0))\n",
    "\n",
    "    # TODO: Add your code to compute the log-likelihood if the hypothesis contains all of the data.\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation - Making predictions\n",
    "We now have all the code in place to make Bayesian predictions regarding the membership of new numbers, as described by the previous equations,\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\in C\\ |\\ X) = \\sum_{h \\in H} P(y \\in C\\ |\\ h) P(h|X).\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 4 (10 points) </h3>\n",
    "<br>\n",
    "<ul>\n",
    "    <li>Fill in the missing code below to help complete the `bayesian_predictions` function.</li>\n",
    "    <li>Use the `bayesian_predictions` function to double check your answer in Problem 2. Remember that there are only two hypotheses \"multiples of 10\" and \"even numbers\". For $X$, the numbers 10 and 30 were observed. Compute the probability that 40 and 4 are a member of the same concept as the numbers in $X$. Don't forget to convert your individual hypotheses to numpy arrays using `convert_h_list_to_numpy` </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_predictions(data_eval, data, list_hypothesis, log_prior):\n",
    "    # INPUT\n",
    "    #  data_eval : [length ne python list] of new numbers we want to check the probability of membership for\n",
    "    #     each number in data_eval is to be evaluated independently -- it's a separate 'y' in equation above\n",
    "    #  data : [python list] observed numbers (X) \n",
    "    #  list_hypothesis : python list of hypotheses, each is a binary numpy array \n",
    "    #  log_prior : numpy vector [length nh] which is the log prior value for each hypothesis\n",
    "    # \n",
    "    # RETURN\n",
    "    #  pp : numpy vector [size ne] of predicted probabilities of new numbers in data_eval (NOTE: NOT IN LOG SPACE)\n",
    "    lpost = log_posterior(data,list_hypothesis,log_prior)\n",
    "    post = np.exp(lpost) # posterior probabilities\n",
    "    h_mat = np.array(list_hypothesis) # create a [nh by x_max] numpy matrix, showing numbers in each hypothesis\n",
    "    ne = len(data_eval) # how many numbers to evaluate\n",
    "    pp = np.zeros(ne) # predicted probability of each number\n",
    "    for idx,de in enumerate(data_eval):\n",
    "        #TODO : Add your code here to compute predicted probabilities. Can be a single line with form \"pp[idx] = \"..\n",
    "    return pp\n",
    "\n",
    "# TODO : Check your answers for problem 2 using the bayesian_prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model and predicting human data\n",
    "Now we have all the pieces in place to run the complete number game model.\n",
    "\n",
    "First, let's described the data from participants. Tenenbaum ran eight participants in an experiment where they were provided with various sets $X$ of random positive examples from a concept. They were asked to rate the probability that each of 30 test numbers would belong to the same concept of the observed examples. \n",
    "\n",
    "The following plot shows the mean rating across the human participants for three different sets. Note that since only 30 test numbers were evaluated, and thus a value of 0 in the plot indicates missing data (rather than zero probability).\n",
    "<img src=\"images/number_game_human.jpeg\" style=\"width: 800px;\"/>\n",
    "\n",
    "Let's produce the same plots for the Bayesian concept learning model using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(x_eval,mypred):\n",
    "    mybottom = -0.1\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.bar(x_eval,mypred-mybottom,bottom=mybottom)\n",
    "    plt.ylim((mybottom,1.2))\n",
    "    plt.xticks(np.arange(0, x_max+1, step=4))\n",
    "    plt.yticks([0,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_all,log_prior_all = generate_hypotheses(mylambda=2./3)\n",
    "x_eval = [2,4,6,8,9,10]+list(range(12,23))+[24,25,26,28,32,36,41,56,62,64,87,95,96]\n",
    "\n",
    "mypred = bayesian_predictions(x_eval, [16], H_all, log_prior_all)\n",
    "plot_predictions(x_eval,mypred)\n",
    "plt.title('X=[16]')\n",
    "plt.ylabel('prob. of membership')\n",
    "mypred = bayesian_predictions(x_eval, [16, 8, 2, 64], H_all, log_prior_all)\n",
    "plot_predictions(x_eval,mypred)\n",
    "plt.title('X=[16, 8, 2, 64]')\n",
    "mypred = bayesian_predictions(x_eval, [16, 23, 19, 20], H_all, log_prior_all)\n",
    "plot_predictions(x_eval,mypred)\n",
    "plt.title('X=[16, 23, 19, 20]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 5 (10 points) </h3>\n",
    "<br>\n",
    "Using the code above, produce plots the show the Bayesian model's predictions. Produce the plots for the default `mylambda = 2./3`, but also reproduce the plots below for another value of `mylambda` that shows qualitatively different results. \n",
    "<ul>\n",
    "    <li>Which value of `mylambda` seems to fit the data best?</li>\n",
    "    <li>Comment on why the predictions change for different values of `mylambda`.</li>\n",
    "</ul>\n",
    "<br>\n",
    "Your response in the cells below should include plots for a least one new setting of lambda, and 1-2 paragraphs of discussion about how the results change as a function of `mylambda`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 6 (10 points) </h3>\n",
    "<br>\n",
    "Discuss your general thoughts on this Bayesian model to understand human judgments in the number game. Discussion questions could include the following (as well as others):\n",
    "<ul>\n",
    "    <li>Is the model convincing? Why or why not?</li>\n",
    "    <li>Is the number game and Bayesian model relevant to more naturalistic settings for concept learning in childhood or everyday life?</li>\n",
    "    <li>Where could the hypothesis space come from?</li>\n",
    "    <li>What algorithms could people be using to approximate Bayesian inference, rather than enumerating all the hypotheses, as in the current implementation?</li>\n",
    "</ul>\n",
    "<br>\n",
    "Please write a short response in the cell below. Your response should be about two paragraphs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h3> Problem 7 (20 points)</h3>\n",
    "<br>\n",
    "This problem asks you to implement a \"likelihood weighted sampler\" as discussed in lecture. Review the lecture notes on importance sampling and specifically \"likelihood weighted sampling\", where we choose the approximate distribution Q to be the prior distribution over hypotheses.\n",
    "<ul>\n",
    "    <li>Fill in the missing code below to help complete the `draw_prior_samples`, `weight_samples`, and `importance_sampler_predictions` functions.</li>\n",
    "    <li>Run your likelihood weighted sampler for 2000 samples and reproduce the plots in Problem 5. Does approximate inference match the exact inference?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prior_samples(nsamp):\n",
    "    # INPUT\n",
    "    #  nsamp : number of hypotheses to be sampled from the prior\n",
    "    #\n",
    "    # RETURN\n",
    "    #  list_H : [nsamp length python list] of sampled hypotheses (each is a binary numpy vector [length x_max])\n",
    "    #\n",
    "    # TODO: Add your code to draw a list of samples (from H_all) given their prior probabilities (log_prior_all)\n",
    "    return list_H\n",
    "\n",
    "def weight_samples(data, list_H):\n",
    "    # INPUT\n",
    "    #  data : [python list] of observed data\n",
    "    #  list_H : [nsamp length pythong list] of sampled hypotheses (each is a binary numpy vector [length x_max])\n",
    "    #     these are the sampled \"particles\" in the importance sampler\n",
    "    #\n",
    "    # Output\n",
    "    #  log_wt : numpy vector [size nsamp] log importance weight of each sample in list_H\n",
    "    #\n",
    "    # TODO: Add your code to return the log-weight of each particle\n",
    "    return log_wt\n",
    "\n",
    "def importance_sampler_predictions(data_eval, list_H, log_wt):\n",
    "    # INPUT\n",
    "    #  data_eval : [length ne python list] of new numbers we want to evaluate the probability of membership for \n",
    "    #     each number in data_eval is to be evaluated independently (as in bayesian_predictions function)\n",
    "    #  list_H : nsamp length pythong list] of sampled hypotheses (each is a binary numpy vector [length x_max])\n",
    "    #  log_wt : numpy vector [size nsamp] log importance weight of each sample in list_H\n",
    "    # \n",
    "    # RETURN\n",
    "    #  pp : numpy vector [size ne] of predicted probabilities of new numbers  (NOTE: NOT IN LOG SPACE)\n",
    "    wt = np.exp(log_wt)\n",
    "    h_mat = np.array(list_H) # create a [nsamp by x_max] numpy matrix, showing numbers in each hypothesis/sample\n",
    "    ne = len(data_eval) # how many numbers to evaluate\n",
    "    pp = np.zeros(ne) # predicted probability of each number\n",
    "    for idx,de in enumerate(data_eval):\n",
    "        #TODO : Add your code here/ Can be a single line with form \"pp[idx] = \"..\n",
    "    return pp\n",
    "    \n",
    "nsamples_importance = 2000 # number of samples\n",
    "plt.figure()\n",
    "list_H_importance = draw_prior_samples(nsamples_importance) # prior samples can be re-used across queries\n",
    "log_wt_importance = weight_samples([16],list_H_importance)\n",
    "mypred = importance_sampler_predictions(x_eval, list_H_importance, log_wt_importance)\n",
    "plot_predictions(x_eval,mypred)\n",
    "plt.title('X=[16] (importance sampler)')\n",
    "plt.ylabel('prob. of membership')\n",
    "\n",
    "log_wt_importance = weight_samples([16, 8, 2, 64],list_H_importance)\n",
    "mypred = importance_sampler_predictions(x_eval, list_H_importance, log_wt_importance)\n",
    "plot_predictions(x_eval,mypred)\n",
    "plt.title('X=[16, 8, 2, 64] (importance sampler)')\n",
    "\n",
    "log_wt_importance = weight_samples([16, 23, 19, 20],list_H_importance)\n",
    "mypred = importance_sampler_predictions(x_eval, list_H_importance, log_wt_importance)\n",
    "plot_predictions(x_eval,mypred)\n",
    "plt.title('X=[16, 23, 19, 20] (importance sampler)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in homework\n",
    "\n",
    "When you are finished with this notebook. Save your work in order to turn it in.  To do this select *File*->*Download As...*->*PDF*. In some cases you may need to save as HTML first, then PDF, to get everything to display in the best way.\n",
    "\n",
    "<img src=\"images/save-pdf.png\" width=\"300\">\n",
    "\n",
    "You should turn in your homework using gradescope. **Note  there are additional parts of this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
